{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d10da6-4073-40c8-8098-ccc759417552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tic/miniconda3/envs/cvpr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "# from abc import ABC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "from scipy.ndimage import label\n",
    "import numpy as np\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "from lightning.fabric.fabric import _FabricOptimizer\n",
    "\n",
    "import torchvision\n",
    "from box import Box\n",
    "from datasets import call_load_dataset\n",
    "from utils.model import Model\n",
    "from utils.losses import DiceLoss, FocalLoss, Matching_Loss\n",
    "from utils.eval_utils import AverageMeter, validate, get_prompts, calc_iou, validate_sam2\n",
    "from utils.tools import copy_model, create_csv, reduce_instances\n",
    "from utils.utils import *\n",
    "\n",
    "import  csv, copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import map_coordinates\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from box import Box\n",
    "from utils.model import Model\n",
    "from utils.sample_utils import get_point_prompts\n",
    "from utils.tools import write_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d122bc-4b7f-4109-91d1-dc08774e6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Optimizer and Scheduler Configuration\n",
    "\n",
    "# %% [code]\n",
    "def configure_opt(cfg: Box, model: Model):\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.model.parameters(),\n",
    "                                 lr=cfg.opt.learning_rate,\n",
    "                                 weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14bca67-28d6-4a38-809b-ab561295c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Configuration Loading and Launch\n",
    "\n",
    "# %% [code]\n",
    "# Example: set arguments manually here\n",
    "# Replace with your config module path, e.g. \"configs.default_config\"\n",
    "import importlib\n",
    "\n",
    "CFG_MODULE = \"configs.config_nwpu\"\n",
    "cfg_module = importlib.import_module(CFG_MODULE)\n",
    "cfg = cfg_module.cfg\n",
    "\n",
    "# Manually merge updates if needed\n",
    "cfg.out_dir = \"./outputs\"\n",
    "cfg.resume = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cf5fee-e992-4e4a-8bbe-bf5d0eb708e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_instance_overlap(seg_tensor: torch.Tensor, image: np.ndarray, alpha=1):\n",
    "    \"\"\"\n",
    "    Overlay colored instance masks only on masked regions of the image.\n",
    "\n",
    "    Args:\n",
    "        seg_tensor (torch.Tensor): Segmentation tensor of shape (N, 1, H, W),\n",
    "                                   where N = number of instances.\n",
    "        image (np.ndarray): Original image, shape (H, W, 3) or (H, W).\n",
    "        alpha (float): Transparency for overlay (0 = transparent, 1 = opaque).\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy and squeeze\n",
    "    seg_np = seg_tensor.squeeze(1).cpu().numpy()  # shape: (N, H, W)\n",
    "    num_instances, H, W = seg_np.shape\n",
    "\n",
    "    # Normalize image if needed\n",
    "    img = image.astype(np.float32)\n",
    "    if img.max() > 1:\n",
    "        img = img / 255.0\n",
    "    if img.ndim == 2:  # grayscale â†’ RGB\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "\n",
    "    # Copy image to draw overlays\n",
    "    overlay = img.copy()\n",
    "\n",
    "    # Generate random distinct colors for each instance\n",
    "    rng = np.random.default_rng(42)\n",
    "    colors = rng.uniform(0, 1, size=(num_instances, 3))\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        mask = seg_np[i] > 0.5  # binary mask\n",
    "        color = colors[i]\n",
    "\n",
    "        # Apply color only to masked regions (blend only on mask)\n",
    "        for c in range(3):\n",
    "            overlay[..., c][mask] = (\n",
    "                (1 - alpha) * img[..., c][mask] + alpha * color[c]\n",
    "            )\n",
    "\n",
    "    return overlay\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# def visualize_points_and_boxes(img_np, prompts, H, W, num_instances):\n",
    "#     \"\"\"\n",
    "#     Draw image with:\n",
    "#       1) Red points (positive only)\n",
    "#       2) Green bounding boxes (separately)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Convert bounding boxes to pixel scale if normalized\n",
    "#     # if bboxes.max() <= 1.5:\n",
    "#     #     bboxes_scaled = bboxes.clone()\n",
    "#     #     bboxes_scaled[:, [0, 2]] *= W\n",
    "#     #     bboxes_scaled[:, [1, 3]] *= H\n",
    "#     # else:\n",
    "#     #     bboxes_scaled = bboxes\n",
    "\n",
    "#     # --- Get prompt coordinates and labels\n",
    "#     try:\n",
    "#         point_coords = prompts[0][0].detach().cpu().numpy()  # [N, 2]\n",
    "#         point_labels = prompts[0][1].detach().cpu().numpy()  # [N,]\n",
    "#         # scale normalized coordinates if needed\n",
    "#         if point_coords.max() <= 1.5:\n",
    "#             point_coords_px = point_coords.copy()\n",
    "#             point_coords_px[:, 0] *= W\n",
    "#             point_coords_px[:, 1] *= H\n",
    "#         else:\n",
    "#             point_coords_px = point_coords\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not extract prompts: {e}\")\n",
    "#         point_coords_px, point_labels = np.empty((0, 2)), np.empty((0,))\n",
    "\n",
    "#     # --- (1) IMAGE + POSITIVE POINTS (RED)\n",
    "#     fig1, ax1 = plt.subplots(figsize=(10, 10))\n",
    "#     ax1.imshow(img_np)\n",
    "#     pos_points = point_coords_px[point_labels == 1]\n",
    "#     if len(pos_points) > 0:\n",
    "#         ax1.scatter(\n",
    "#             pos_points[:, 0], pos_points[:, 1],\n",
    "#             c='red', s=100, marker='o', edgecolors='white',\n",
    "#             linewidths=1.5, label='Positive Points', zorder=5\n",
    "#         )\n",
    "#     ax1.set_title(\"Image with Positive Points (Red)\")\n",
    "#     ax1.axis(\"off\")\n",
    "#     if len(pos_points) > 0:\n",
    "#         ax1.legend(loc='upper right')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # --- (2) IMAGE + BOUNDING BOXES (GREEN)\n",
    "#     fig2, ax2 = plt.subplots(figsize=(10, 10))\n",
    "#     ax2.imshow(img_np)\n",
    "#     # for idx, box in enumerate(bboxes_scaled):\n",
    "#     #     x1, y1, x2, y2 = box.tolist()\n",
    "#     #     width, height = x2 - x1, y2 - y1\n",
    "#     #     rect = Rectangle(\n",
    "#     #         (x1, y1), width, height,\n",
    "#     #         linewidth=2, edgecolor='lime', facecolor='none',\n",
    "#     #         label='Bounding Box' if idx == 0 else \"\"\n",
    "#     #     )\n",
    "#     #     ax2.add_patch(rect)\n",
    "#     ax2.set_title(f\"Image with Bounding Boxes (Green) â€” {num_instances} instances\")\n",
    "#     ax2.axis(\"off\")\n",
    "#     handles, labels = ax2.get_legend_handles_labels()\n",
    "#     if handles:\n",
    "#         ax2.legend(handles, labels, loc='upper right')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Safe backend for headless / script mode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_points(img_np, prompts, H, W, show=True):\n",
    "    \"\"\"\n",
    "    Draw image with:\n",
    "      ðŸ”´ Red positive points only.\n",
    "    Returns:\n",
    "      img_points_vis (np.ndarray): RGB image with drawn points.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Extract prompt coordinates safely ---\n",
    "    try:\n",
    "        point_coords = prompts[0][0].detach().cpu().numpy()  # [N, 2]\n",
    "        point_labels = prompts[0][1].detach().cpu().numpy()  # [N,]\n",
    "        # scale normalized coordinates if needed\n",
    "        if point_coords.max() <= 1.5:\n",
    "            point_coords_px = point_coords.copy()\n",
    "            point_coords_px[:, 0] *= W\n",
    "            point_coords_px[:, 1] *= H\n",
    "        else:\n",
    "            point_coords_px = point_coords\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract prompts: {e}\")\n",
    "        point_coords_px, point_labels = np.empty((0, 2)), np.empty((0,))\n",
    "\n",
    "    # --- Draw positive points ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img_np)\n",
    "    pos_points = point_coords_px[point_labels == 1]\n",
    "\n",
    "    if len(pos_points) > 0:\n",
    "        ax.scatter(\n",
    "            pos_points[:, 0], pos_points[:, 1],\n",
    "            c='red', s=100, marker='o', edgecolors='white',\n",
    "            linewidths=1.5, zorder=5\n",
    "        )\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    ax.set_title(\"Image with Positive Points (Red)\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # --- Convert figure to a NumPy image safely ---\n",
    "    fig.canvas.draw()\n",
    "    img_rgba = np.asarray(fig.canvas.buffer_rgba())\n",
    "    img_points_vis = img_rgba[:, :, :3].copy()  # drop alpha channel\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "    return img_points_vis\n",
    "\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_per_instance(img_np, gt_masks_np, entropy_maps_np, preds_np, pseudo_lab):\n",
    "    \"\"\"\n",
    "    Visualize per-instance segmentation details:\n",
    "      - Ground truth mask (overlay)\n",
    "      - Entropy map\n",
    "      - Prediction mask (overlay)\n",
    "    Then show global prediction + pseudo labels.\n",
    "    \"\"\"\n",
    "    num_instances = gt_masks_np.shape[0]\n",
    "    H, W = gt_masks_np.shape[-2:]\n",
    "    print(f\"ðŸ”¹ Visualizing {num_instances} instances, each {H}Ã—{W}\")\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        gt_mask = gt_masks_np[i]\n",
    "        entropy = entropy_maps_np[i][0]\n",
    "        pred_mask = preds_np[i][0]\n",
    "\n",
    "        # Normalize entropy to [0,1]\n",
    "        entropy_norm = (entropy - entropy.min()) / (entropy.max() - entropy.min() + 1e-8)\n",
    "\n",
    "        # === GT mask overlay ===\n",
    "        gt_mask_rgb = cv2.applyColorMap((gt_mask * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        gt_mask_rgb = cv2.cvtColor(gt_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        gt_overlay = cv2.addWeighted(gt_mask_rgb, 0.5, img_np, 0.5, 0)\n",
    "\n",
    "        # === Entropy heatmap ===\n",
    "        entropy_heatmap = cv2.applyColorMap((entropy_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        entropy_heatmap = cv2.cvtColor(entropy_heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # === Pred mask overlay ===\n",
    "        pred_mask_rgb = cv2.applyColorMap((pred_mask * 255).astype(np.uint8), cv2.COLORMAP_PARULA)\n",
    "        pred_mask_rgb = cv2.cvtColor(pred_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        pred_overlay = cv2.addWeighted(pred_mask_rgb, 0.5, img_np, 0.5, 0)\n",
    "\n",
    "        # --- Plot instance summary ---\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 12))\n",
    "        fig.suptitle(f\"Instance {i+1}\", fontsize=18)\n",
    "\n",
    "        axes[0].imshow(gt_overlay)\n",
    "        axes[0].set_title(\"GT Mask Overlay\", fontsize=14)\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(entropy_heatmap)\n",
    "        axes[1].set_title(\"Entropy Map\", fontsize=14)\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(pred_overlay)\n",
    "        axes[2].set_title(\"Prediction Overlay\", fontsize=14)\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Global visualization ---\n",
    "    print(\"âœ… Global Prediction & Pseudo Labels\")\n",
    "\n",
    "    # Combine predictions across instances\n",
    "    pred_all = (preds_np > 0.5).sum(axis=0)[0]\n",
    "    pseudo_np = (pseudo_lab[0].detach().cpu().numpy() > 0.5)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 12))\n",
    "    axes[0].imshow(pred_all, cmap='gray')\n",
    "    axes[0].set_title(\"Combined Predictions (>0.5)\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pseudo_np, cmap='gray')\n",
    "    axes[1].set_title(\"Pseudo Labels (>0.5)\", fontsize=14)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "idx = 1\n",
    "idx = 1\n",
    "def visualize_instance_segmentation(images, gt_masks_new,prompts, preds,idx):\n",
    "\n",
    "\n",
    "    # --- Convert image tensor to numpy\n",
    "    img = images[0].detach().cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    if img.min() < 0:\n",
    "        img = img * std + mean\n",
    "    img_np = TF.to_pil_image(img.clamp(0, 1))\n",
    "    img_np = img_np.resize((1024, 1024), Image.BILINEAR)\n",
    "    \n",
    "    img_np = np.array(img_np)\n",
    "    \n",
    "    \n",
    "    # Function to convert matplotlib fig to np image\n",
    "    def fig_to_np(fig):\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        return img\n",
    "    \n",
    "    # Example index\n",
    "      # change this dynamically in your loop\n",
    "    \n",
    "    # === Visualize and save ===\n",
    "    \n",
    "    # 1ï¸âƒ£ Visualize points (input image with prompts)\n",
    "    H, W = gt_masks_new[0].shape[-2:]\n",
    "    img_vis = visualize_points(img_np, prompts, H, W, show=False)\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.imshow(img_vis)\n",
    "    ax1.axis('off')\n",
    "    fig1.savefig(os.path.join(save_dir, f\"{idx:03d}_a_img_vis.png\"), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # 2ï¸âƒ£ Overlay Ground Truth\n",
    "    overlay_gt = plot_instance_overlap(gt_masks_new[0], img_np)\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.imshow(overlay_gt)\n",
    "    ax2.axis('off')\n",
    "    fig2.savefig(os.path.join(save_dir, f\"{idx:03d}_b_gt.png\"), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # 3ï¸âƒ£ Overlay Prediction\n",
    "    overlay_pred = plot_instance_overlap(preds.squeeze(1), img_np)\n",
    "    fig3, ax3 = plt.subplots()\n",
    "    ax3.imshow(overlay_pred)\n",
    "    ax3.axis('off')\n",
    "    fig3.savefig(os.path.join(save_dir, f\"{idx:03d}_c_{method}.png\"), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig3)\n",
    "    idx+=1\n",
    "        \n",
    "\n",
    "\n",
    "    # pseudo_lab = torch.sigmoid(torch.stack(soft_masks, dim=0)).sum(dim=1)\n",
    "    # visualize_per_instance(img_np, gt_masks_np, entropy_maps_np, preds_np, pseudo_lab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0400cd0a-8eda-4fa6-b963-88ca80d6d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "\n",
    "idx = 1\n",
    "\n",
    "def visualsize_overlapa(images, gt_masks_new, prompts, preds,idx, save_dir, method=\"pred_overlap\"):\n",
    "\n",
    "    # --- Convert image tensor to numpy\n",
    "    img = images[0].detach().cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    if img.min() < 0:\n",
    "        img = img * std + mean\n",
    "    img_np = TF.to_pil_image(img.clamp(0, 1))\n",
    "    img_np = img_np.resize((1024, 900), Image.BILINEAR)\n",
    "    img_np = np.array(img_np)\n",
    "\n",
    "    \n",
    "\n",
    "    # === Compute overlap across instances ===\n",
    "    # preds assumed shape: [num_instances, H, W]\n",
    "    instance_masks = preds.squeeze(1).cpu().numpy()  # [N, H, W]\n",
    "    if instance_masks.ndim == 2:\n",
    "        instance_masks = np.expand_dims(instance_masks, axis=0)  # handle single instance case\n",
    "    mean_thresh = 0.5\n",
    "   \n",
    "    pred_binary = (((instance_masks)>mean_thresh) )\n",
    "    pred_binary = torch.tensor(pred_binary)  # convert numpy array to tensor\n",
    "    overlap_count = pred_binary.sum(dim=0)   # now dim works\n",
    "    overlap_mask = (overlap_count > 1)\n",
    "    \n",
    "    overlap_count = pred_binary.sum(dim=0)\n",
    "    overlap_mask = (overlap_count > 1)\n",
    "\n",
    "\n",
    "\n",
    "    total_foreground = (pred_binary > 0).sum()\n",
    "    overlap_pixels = overlap_mask.sum()\n",
    "    overlap_ratio = overlap_pixels / (total_foreground + 1e-8)\n",
    "    print(overlap_ratio)\n",
    "    if overlap_ratio > 0.01:\n",
    "\n",
    "        # === Visualize input prompts ===\n",
    "        H, W = gt_masks_new[0].shape[-2:]\n",
    "        img_vis = visualize_points(img_np, prompts, H, W, show=False)\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.imshow(img_vis)\n",
    "        ax1.axis('off')\n",
    "        fig1.savefig(os.path.join(save_dir, f\"{idx:03d}_a_img_vis.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig1)\n",
    "        \n",
    "        # === Overlay prediction ===\n",
    "        overlay_pred = plot_instance_overlap(preds.squeeze(1), img_np)\n",
    "        fig3, ax3 = plt.subplots()\n",
    "        ax3.imshow(overlay_pred)\n",
    "        ax3.axis('off')\n",
    "        fig3.savefig(os.path.join(save_dir, f\"{idx:03d}_c_{method}.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig3)\n",
    "\n",
    "        \n",
    "        # # Keep only pixels with >50% overlap\n",
    "        # overlap_mask = (overlap_map / num_instances) > overlap_threshold\n",
    "    \n",
    "        # --- Overlay overlap mask on input image ---\n",
    "        overlay_img = img_np.copy()\n",
    "        overlay_color = np.array([255, 0, 0], dtype=np.uint8)  # Red for overlap\n",
    "        overlay_img[overlap_mask] = (0.5 * overlay_img[overlap_mask] + 0.5 * overlay_color).astype(np.uint8)\n",
    "    \n",
    "        # Save overlap visualization\n",
    "        fig4, ax4 = plt.subplots()\n",
    "        ax4.imshow(overlay_img)\n",
    "        ax4.axis('off')\n",
    "        fig4.savefig(os.path.join(save_dir, f\"{idx:03d}_overlap.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig4)\n",
    "    \n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d15e71-00bb-478c-84ee-87d7828c408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "def validate_w_show(fabric: L.Fabric, cfg: Box, model: Model, val_dataloader: DataLoader, name: str, epoch: int = 0):\n",
    "    model.eval()\n",
    "    ious = AverageMeter()\n",
    "    f1_scores = AverageMeter()\n",
    "    recall = AverageMeter()\n",
    "    precision = AverageMeter()\n",
    "    idx = 1\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(tqdm(val_dataloader, desc='Validation', ncols=100)):\n",
    "            images, bboxes, gt_masks, img_paths = data\n",
    "            num_images = images.size(0)\n",
    "            prompts = get_prompts(cfg, bboxes, gt_masks)\n",
    "\n",
    "            _, pred_masks, _, _ = model(images, prompts)\n",
    "\n",
    "            \n",
    "            # visualsize_overlapa(images, gt_masks,prompts,  pred_masks[0], idx,save_dir)\n",
    "            \n",
    "            \n",
    "            visualize_instance_segmentation(images, gt_masks,prompts,  pred_masks[0], idx)\n",
    "            for pred_mask, gt_mask in zip(pred_masks, gt_masks):\n",
    "                batch_stats = smp.metrics.get_stats(\n",
    "                    pred_mask,\n",
    "                    gt_mask.int(),\n",
    "                    mode='binary',\n",
    "                    threshold=0.5,\n",
    "                )\n",
    "                batch_recall = smp.metrics.recall(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                batch_precision = smp.metrics.precision(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                batch_iou = smp.metrics.iou_score(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                batch_f1 = smp.metrics.f1_score(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                ious.update(batch_iou, num_images)\n",
    "                f1_scores.update(batch_f1, num_images)\n",
    "                recall.update(batch_recall, num_images)\n",
    "                precision.update(batch_precision, num_images)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            idx+=1\n",
    "            if idx > 7:\n",
    "                break\n",
    "\n",
    "    fabric.print(\n",
    "        f'Val: [{epoch}] - [{iter+1}/{len(val_dataloader)}]: IoU: [{ious.avg:.4f}] -- Recall: [{recall.avg:.4f}] -- Precision [{precision.avg:.4f}] -- F1: [{f1_scores.avg:.4f}]'\n",
    "    )\n",
    "    csv_dict = {\"Prompt\": cfg.prompt, \"IoU\": f\"{ious.avg:.4f}\",\"Recall\": f\"{recall.avg:.4f}\", \"Precision\": f\"{precision.avg:.4f}\", \"F1\": f\"{f1_scores.avg:.4f}\", \"epoch\": epoch}\n",
    "\n",
    "    if fabric.global_rank == 0:\n",
    "        write_csv(os.path.join(cfg.out_dir, \"metrics.csv\"), csv_dict, csv_head=cfg.csv_keys)\n",
    "    return ious.avg, f1_scores.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b77050-93ee-48b1-9b52-673b96f45e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "./pretrained_all/nwpu/resam/best_model.pth\n",
      "Resumed from explicit checkpoint: \n"
     ]
    }
   ],
   "source": [
    "gpu_ids = [str(i) for i in range(torch.cuda.device_count())]\n",
    "num_devices = len(gpu_ids)\n",
    "fabric = L.Fabric(accelerator=\"auto\",\n",
    "                  devices=num_devices,\n",
    "                  strategy=\"auto\",\n",
    "                  loggers=[TensorBoardLogger(cfg.out_dir)])\n",
    "fabric.launch()\n",
    "fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    create_csv(os.path.join(cfg.out_dir, \"metrics.csv\"), csv_head=cfg.csv_keys)\n",
    "\n",
    "with fabric.device:\n",
    "    model = Model(cfg)\n",
    "    model.setup()\n",
    "\n",
    "load_datasets = call_load_dataset(cfg)\n",
    "train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt = True)\n",
    "train_data = fabric._setup_dataloader(train_data)\n",
    "val_data = fabric._setup_dataloader(val_data)\n",
    "pt_data = fabric._setup_dataloader(pt_data)\n",
    "optimizer, scheduler = configure_opt(cfg, model)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "auto_ckpt = './pretrained_all/nwpu/resam/best_model.pth'#\n",
    "method = \"sam\"\n",
    "data_= \"nwpu\"\n",
    "save_dir = \"results\"+f\"/{data_}/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(auto_ckpt)\n",
    "if auto_ckpt is not None:\n",
    "    full_checkpoint = fabric.load(auto_ckpt)\n",
    "\n",
    "    if isinstance(full_checkpoint, dict) and \"model\" in full_checkpoint:\n",
    "        model.load_state_dict(full_checkpoint[\"model\"])\n",
    "        if \"optimizer\" in full_checkpoint:\n",
    "            optimizer.load_state_dict(full_checkpoint[\"optimizer\"])\n",
    "    else:\n",
    "        model.load_state_dict(full_checkpoint)\n",
    "    loaded = True\n",
    "    fabric.print(f\"Resumed from explicit checkpoint: {cfg.model.ckpt}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f968d0-5eff-439d-94bf-27c5bcf900a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[92mDirect test on the original SAM.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                                                           | 0/130 [00:00<?, ?it/s]/tmp/ipykernel_295927/3157961382.py:169: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend(loc='upper right')\n",
      "Validation:   5%|â–ˆâ–ˆâ–Ž                                                | 6/130 [01:22<28:22, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: [0] - [7/130]: IoU: [0.3103] -- Recall: [0.9365] -- Precision [0.3205] -- F1: [0.4393]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init_iou = 0\n",
    "idx = 1\n",
    "print('-'*100)\n",
    "    \n",
    "print('\\033[92mDirect test on the original SAM.\\033[0m') \n",
    "init_iou, _, = validate_w_show(fabric, cfg, model, val_data, name=cfg.name, epoch=0)\n",
    "print('-'*100)\n",
    "del _    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6be28e-8997-42ee-b750-94fce52ca507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc6468-29b6-466a-9c57-370c085eec7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3094aa-6d01-41b6-9513-57a1845a778d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a58283-68d4-47fd-8b4f-631ee4fbbfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ca0b0-925d-4ac9-9c58-b96ee9374fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa67cd-df52-4009-a854-4297b9e7bfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8d80b-1b61-4b32-834d-8d9d63851aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b9336-0869-4cf1-8095-a501ab0c5db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404461ea-6c1b-4800-a15d-9bb3b7c48422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4fe91-052f-4b73-9459-3f07a212f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f6cf1-90d6-43f3-8c6c-dca70521fa51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00effa6b-c4d2-4e7d-a02f-e0fad14fbcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b57c27-ed2b-449b-95cc-61d068d98975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forward(img_tensor, prompt, model):\n",
    "    with torch.no_grad():\n",
    "        _, masks_pred, _, _ = model(img_tensor, prompt)\n",
    "    entropy_maps = []\n",
    "    pred_ins = []\n",
    "    eps=1e-8\n",
    "    for i, mask_p in enumerate( masks_pred[0]):\n",
    "        mask_p = torch.sigmoid(mask_p)\n",
    "        p = mask_p.clamp(1e-6, 1 - 1e-6)\n",
    "        if p.ndim == 2:\n",
    "            p = p.unsqueeze(0)\n",
    "\n",
    "        # entropy_map = entropy_map_calculate(p)\n",
    "        entropy = - (p * torch.log(p + eps) + (1 - p) * torch.log(1 - p + eps))\n",
    "        max_ent = torch.log(torch.tensor(2.0, device=mask_p.device))\n",
    "        entropy_norm = entropy / (max_ent + 1e-8)   # [0, 1]\n",
    "        entropy_maps.append(entropy_norm)\n",
    "        pred_ins.append(p)\n",
    "\n",
    "    return entropy_maps, pred_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194d20e1-9b1c-4e2c-8a57-a8277feb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_map_calculate(p):\n",
    "    eps = 1e-8\n",
    "    p = p.clamp(eps, 1 - eps)  # Safe!\n",
    "    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    # entropy_map = entropy_map.max(dim=0)[0]\n",
    "    return entropy_map# / torch.log(torch.tensor(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117b8954-1e30-42ac-8e76-95a009fb352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_feature(embedding_map, bbox, stride=16, pooling='avg'):\n",
    "    \"\"\"\n",
    "    Extract a feature vector from an embedding map given a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        embedding_map (torch.Tensor): Shape (C, H_feat, W_feat) or (B, C, H_feat, W_feat)\n",
    "        bbox (list or torch.Tensor): [x1, y1, x2, y2] in original image coordinates\n",
    "        stride (int): Downscaling factor between image and feature map\n",
    "        pooling (str): 'avg' or 'max' pooling inside the bbox region\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Feature vector of shape (C,)\n",
    "    \"\"\"\n",
    "    # If batch dimension exists, assume batch size 1\n",
    "    if embedding_map.dim() == 4:\n",
    "        embedding_map = embedding_map[0]\n",
    "\n",
    "    C, H_feat, W_feat = embedding_map.shape\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    # Map bbox to feature map coordinates\n",
    "    fx1 = max(int(x1 / stride), 0)\n",
    "    fy1 = max(int(y1 / stride), 0)\n",
    "    fx2 = min(int((x2 + stride - 1) / stride), W_feat)  # ceil division\n",
    "    fy2 = min(int((y2 + stride - 1) / stride), H_feat)\n",
    "\n",
    "    # Crop the feature map to bbox region\n",
    "    region = embedding_map[:, fy1:fy2, fx1:fx2]\n",
    "\n",
    "    if region.numel() == 0:\n",
    "        # fallback to global feature if bbox is too small\n",
    "        region = embedding_map\n",
    "\n",
    "    # Pool to get a single feature vector\n",
    "    if pooling == 'avg':\n",
    "        feature_vec = region.mean(dim=(1,2))\n",
    "    elif pooling == 'max':\n",
    "        feature_vec = region.amax(dim=(1,2))\n",
    "    else:\n",
    "        raise ValueError(\"pooling must be 'avg' or 'max'\")\n",
    "\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121b5181-f324-45ab-88af-dc296f6eca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_opt(cfg: Box, model: Model):\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    # optimize only trainable params (e.g., LoRA)\n",
    "    trainable_params = (p for p in model.model.parameters() if p.requires_grad)\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=cfg.opt.learning_rate, weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3a1b59-f689-4588-87cd-2a8d8d2238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_loss(features, queue, tau=0.07):\n",
    "    \"\"\"\n",
    "    features: [B, D] current batch embeddings (normalized)\n",
    "    queue: deque of [D] past embeddings (detached)\n",
    "    \"\"\"\n",
    "    if len(queue) == 0:\n",
    "        return torch.tensor(0., device=features.device)\n",
    "\n",
    "    # Stack all past features from queue\n",
    "    with torch.no_grad():\n",
    "        past_feats = torch.stack(list(queue), dim=0)  # [Q, D]\n",
    "\n",
    "    # Normalize\n",
    "    features = F.normalize(features, dim=1)\n",
    "    past_feats = F.normalize(past_feats, dim=1)\n",
    "\n",
    "    # Compute cosine similarities (batch x queue)\n",
    "    logits = torch.mm(features, past_feats.t()) / tau  # [B, Q]\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # Weighted alignment (like SSAL)\n",
    "    cos = (logits * tau).clamp(-1, 1)  # revert scaling, approximate cos\n",
    "    loss = ((1 - cos) * probs).sum(dim=1).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81a173a-6b09-49c4-b54d-79b10737de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_latest_checkpoint(save_dir):\n",
    "    \"\"\"\n",
    "    Look for the most recent .pt/.pth file in save_dir.\n",
    "    Returns absolute path or None if not found.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        return None\n",
    "    ckpt_files = [\n",
    "        os.path.join(save_dir, f)\n",
    "        for f in os.listdir(save_dir)\n",
    "        if f.endswith(\".pt\") or f.endswith(\".pth\")\n",
    "    ]\n",
    "    if not ckpt_files:\n",
    "        return None\n",
    "    ckpt_files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    return ckpt_files[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a369bcfd-3e11-4b0f-85ca-00e9399d73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08d8e08-461c-4082-9d3e-bda1442d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_instance_overlap(seg_tensor: torch.Tensor, image: np.ndarray, alpha=1):\n",
    "    \"\"\"\n",
    "    Overlay colored instance masks only on masked regions of the image.\n",
    "\n",
    "    Args:\n",
    "        seg_tensor (torch.Tensor): Segmentation tensor of shape (N, 1, H, W),\n",
    "                                   where N = number of instances.\n",
    "        image (np.ndarray): Original image, shape (H, W, 3) or (H, W).\n",
    "        alpha (float): Transparency for overlay (0 = transparent, 1 = opaque).\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy and squeeze\n",
    "    seg_np = seg_tensor.squeeze(1).cpu().numpy()  # shape: (N, H, W)\n",
    "    num_instances, H, W = seg_np.shape\n",
    "\n",
    "    # Normalize image if needed\n",
    "    img = image.astype(np.float32)\n",
    "    if img.max() > 1:\n",
    "        img = img / 255.0\n",
    "    if img.ndim == 2:  # grayscale â†’ RGB\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "\n",
    "    # Copy image to draw overlays\n",
    "    overlay = img.copy()\n",
    "\n",
    "    # Generate random distinct colors for each instance\n",
    "    rng = np.random.default_rng(42)\n",
    "    colors = rng.uniform(0, 1, size=(num_instances, 3))\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        mask = seg_np[i] > 0.5  # binary mask\n",
    "        color = colors[i]\n",
    "\n",
    "        # Apply color only to masked regions (blend only on mask)\n",
    "        for c in range(3):\n",
    "            overlay[..., c][mask] = (\n",
    "                (1 - alpha) * img[..., c][mask] + alpha * color[c]\n",
    "            )\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Instance Overlay ({num_instances} instances)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_points_and_boxes(img_np, bboxes, prompts, H, W, num_instances):\n",
    "    \"\"\"\n",
    "    Draw image with:\n",
    "      1) Red points (positive only)\n",
    "      2) Green bounding boxes (separately)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert bounding boxes to pixel scale if normalized\n",
    "    if bboxes.max() <= 1.5:\n",
    "        bboxes_scaled = bboxes.clone()\n",
    "        bboxes_scaled[:, [0, 2]] *= W\n",
    "        bboxes_scaled[:, [1, 3]] *= H\n",
    "    else:\n",
    "        bboxes_scaled = bboxes\n",
    "\n",
    "    # --- Get prompt coordinates and labels\n",
    "    try:\n",
    "        point_coords = prompts[0][0].detach().cpu().numpy()  # [N, 2]\n",
    "        point_labels = prompts[0][1].detach().cpu().numpy()  # [N,]\n",
    "        # scale normalized coordinates if needed\n",
    "        if point_coords.max() <= 1.5:\n",
    "            point_coords_px = point_coords.copy()\n",
    "            point_coords_px[:, 0] *= W\n",
    "            point_coords_px[:, 1] *= H\n",
    "        else:\n",
    "            point_coords_px = point_coords\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract prompts: {e}\")\n",
    "        point_coords_px, point_labels = np.empty((0, 2)), np.empty((0,))\n",
    "\n",
    "    # --- (1) IMAGE + POSITIVE POINTS (RED)\n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 7))\n",
    "    ax1.imshow(img_np)\n",
    "    pos_points = point_coords_px[point_labels == 1]\n",
    "    if len(pos_points) > 0:\n",
    "        ax1.scatter(\n",
    "            pos_points[:, 0], pos_points[:, 1],\n",
    "            c='red', s=100, marker='o', edgecolors='white',\n",
    "            linewidths=1.5, label='Positive Points', zorder=5\n",
    "        )\n",
    "    ax1.set_title(\"Image with Positive Points (Red)\")\n",
    "    ax1.axis(\"off\")\n",
    "    if len(pos_points) > 0:\n",
    "        ax1.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- (2) IMAGE + BOUNDING BOXES (GREEN)\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 7))\n",
    "    ax2.imshow(img_np)\n",
    "    for idx, box in enumerate(bboxes_scaled):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor='lime', facecolor='none',\n",
    "            label='Bounding Box' if idx == 0 else \"\"\n",
    "        )\n",
    "        ax2.add_patch(rect)\n",
    "    ax2.set_title(f\"Image with Bounding Boxes (Green) â€” {num_instances} instances\")\n",
    "    ax2.axis(\"off\")\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        ax2.legend(handles, labels, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_per_instance(img_np, gt_masks_np, entropy_maps_np, preds_np, pseudo_lab):\n",
    "    \"\"\"\n",
    "    Visualize per-instance segmentation details:\n",
    "      - Ground truth mask (overlay)\n",
    "      - Entropy map\n",
    "      - Prediction mask (overlay)\n",
    "    Then show global prediction + pseudo labels.\n",
    "    \"\"\"\n",
    "    num_instances = gt_masks_np.shape[0]\n",
    "    H, W = gt_masks_np.shape[-2:]\n",
    "    print(f\"ðŸ”¹ Visualizing {num_instances} instances, each {H}Ã—{W}\")\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        gt_mask = gt_masks_np[i]\n",
    "        entropy = entropy_maps_np[i][0]\n",
    "        pred_mask = preds_np[i][0]\n",
    "\n",
    "        # Normalize entropy to [0,1]\n",
    "        entropy_norm = (entropy - entropy.min()) / (entropy.max() - entropy.min() + 1e-8)\n",
    "\n",
    "        # === GT mask overlay ===\n",
    "        gt_mask_rgb = cv2.applyColorMap((gt_mask * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        gt_mask_rgb = cv2.cvtColor(gt_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        gt_overlay = cv2.addWeighted(gt_mask_rgb, 0.5, img_np, 0.5, 0)\n",
    "\n",
    "        # === Entropy heatmap ===\n",
    "        entropy_heatmap = cv2.applyColorMap((entropy_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        entropy_heatmap = cv2.cvtColor(entropy_heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # === Pred mask overlay ===\n",
    "        pred_mask_rgb = cv2.applyColorMap((pred_mask * 255).astype(np.uint8), cv2.COLORMAP_PARULA)\n",
    "        pred_mask_rgb = cv2.cvtColor(pred_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        pred_overlay = cv2.addWeighted(pred_mask_rgb, 0.5, img_np, 0.5, 0)\n",
    "\n",
    "        # --- Plot instance summary ---\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 12))\n",
    "        fig.suptitle(f\"Instance {i+1}\", fontsize=18)\n",
    "\n",
    "        axes[0].imshow(gt_overlay)\n",
    "        axes[0].set_title(\"GT Mask Overlay\", fontsize=14)\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(entropy_heatmap)\n",
    "        axes[1].set_title(\"Entropy Map\", fontsize=14)\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(pred_overlay)\n",
    "        axes[2].set_title(\"Prediction Overlay\", fontsize=14)\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Global visualization ---\n",
    "    print(\"âœ… Global Prediction & Pseudo Labels\")\n",
    "\n",
    "    # Combine predictions across instances\n",
    "    pred_all = (preds_np > 0.5).sum(axis=0)[0]\n",
    "    pseudo_np = (pseudo_lab[0].detach().cpu().numpy() > 0.5)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 12))\n",
    "    axes[0].imshow(pred_all, cmap='gray')\n",
    "    axes[0].set_title(\"Combined Predictions (>0.5)\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pseudo_np, cmap='gray')\n",
    "    axes[1].set_title(\"Pseudo Labels (>0.5)\", fontsize=14)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_instance_segmentation(images_weak, gt_masks_new,prompts, entropy_maps, preds,\n",
    "                                    invert_overlap_map, soft_masks, bboxes, j=0, slice_step=50):\n",
    "    \"\"\"\n",
    "    Visualizes:\n",
    "    1. The weak input image\n",
    "    2. Ground-truth instance masks\n",
    "    3. Entropy heatmaps\n",
    "    4. Predicted masks\n",
    "    5. Bounding boxes overlaid on image\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # --- Convert image tensor to numpy\n",
    "    img = images_weak[0].detach().cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    if img.min() < 0:\n",
    "        img = img * std + mean\n",
    "    img_np = TF.to_pil_image(img.clamp(0, 1))\n",
    "    img_np = img_np.resize((1024, 1024), Image.BILINEAR)\n",
    "    \n",
    "    img_np = np.array(img_np)\n",
    "    print(\"DDDDDDDDDDDDDDD\", img_np.shape)\n",
    "    print(f\"Orignal Image\")\n",
    "\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(img_np)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"Ground Truth\")\n",
    "    plot_instance_overlap(gt_masks_new[0], img_np)\n",
    "\n",
    "\n",
    "    print(\"Current Model Output\")\n",
    "    plot_instance_overlap(preds.squeeze(1), img_np)\n",
    "\n",
    "\n",
    "    num_instances = gt_masks_new.shape[1]\n",
    "    H, W = gt_masks_new.shape[-2:]\n",
    "    \n",
    "    visualize_points_and_boxes(img_np, bboxes, prompts, H, W, num_instances)\n",
    "\n",
    "\n",
    "    # pseudo_lab = torch.sigmoid(torch.stack(soft_masks, dim=0)).sum(dim=1)\n",
    "    # visualize_per_instance(img_np, gt_masks_np, entropy_maps_np, preds_np, pseudo_lab)\n",
    "\n",
    "    preds_np = preds.detach().cpu().numpy()\n",
    "    # num_instances = soft_masks[0].shape[0]\n",
    "\n",
    "    soft_masks_np = soft_masks[0]\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    # --- Visualize per-instance details\n",
    "    n_cols = 4\n",
    "    n_rows = min(num_instances, 5)\n",
    "    for i in range(n_rows):\n",
    "        pred_mask = preds_np[i][0]\n",
    "      \n",
    "\n",
    "\n",
    "        print(f\"Instance {i+1} Predicted mask\")\n",
    "        plt.imshow(pred_mask > 0.5, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        soft_mask = soft_masks_np[i]\n",
    "        soft_mask = torch.sigmoid(soft_mask)\n",
    "\n",
    "\n",
    "        print(f\"Pseudo {i+1} mask\")\n",
    "        plt.imshow(soft_mask.detach().cpu().numpy() > 0.5, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        entropy_map = entropy_maps[i]\n",
    "        print(f\"Entropy {i+1} mask\")\n",
    "        plt.imshow(entropy_map[0].detach().cpu().numpy() , cmap='viridis')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"INVERTED overlap  mask\")\n",
    "    plt.imshow((1-invert_overlap_map[0]).detach().cpu().numpy() > 0.5, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    pred_one = (preds_np > 0.5).sum(axis=0)\n",
    "    print(\"Pred all\")\n",
    "    plt.imshow(pred_one[0] > 0.5, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Pseudo labels\")\n",
    "    pseudo_lab = (soft_masks_np.detach().cpu().numpy() > 0.99).sum(axis=0)\n",
    "    print(pseudo_lab.shape)\n",
    "    print(pseudo_lab.max())\n",
    "    plt.imshow((pseudo_lab > 0.99), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Bounding boxes tensor:\", bboxes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b02bfe35-be35-42ce-9f64-76d59fc5b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "feature_queue = deque(maxlen=32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe6694c-1823-4aba-a32a-711f3c718218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam(\n",
    "    cfg: Box,\n",
    "    fabric: L.Fabric,\n",
    "    model: Model,\n",
    "    optimizer: _FabricOptimizer,\n",
    "    scheduler: _FabricOptimizer,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    init_iou, \n",
    "    vis\n",
    "):\n",
    "\n",
    "    \n",
    "    # collected = sort_entropy_(model, target_pts)\n",
    "    focal_loss = FocalLoss()\n",
    "    dice_loss = DiceLoss()\n",
    "    best_ent = 1000000\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    no_improve_count = 0\n",
    "    max_patience = cfg.get(\"patience\", 3)  # stop if no improvement for X validations\n",
    "    match_interval = cfg.match_interval\n",
    "    eval_interval = int(len(train_dataloader) * 1)\n",
    "\n",
    "    window_size = 30\n",
    "\n",
    "    embedding_queue = []\n",
    "    ite_em = 0\n",
    "\n",
    "    # Prepare output dirs\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    csv_path = os.path.join(cfg.out_dir, \"training_log.csv\")\n",
    "\n",
    "    # Initialize CSV\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Epoch\", \"Iteration\", \"Val_ent\", \"Best_ent\", \"Status\"])\n",
    "\n",
    "    fabric.print(f\"Training with rollback enabled. Logging to: {csv_path}\")\n",
    "\n",
    "    entropy_means = deque(maxlen=len(train_dataloader))\n",
    "\n",
    "    eps = 1e-8\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        focal_losses = AverageMeter()\n",
    "        dice_losses = AverageMeter()\n",
    "        iou_losses = AverageMeter()\n",
    "        total_losses = AverageMeter()\n",
    "        match_losses = AverageMeter()\n",
    "        sim_losses = AverageMeter()\n",
    "        end = time.time()\n",
    "        num_iter = len(train_dataloader)\n",
    "\n",
    "        idx = random.randint(0, len(train_dataloader) - 1)\n",
    "        data = train_dataloader.dataset[10]\n",
    "        print(idx)\n",
    "\n",
    "        for iter, data in enumerate(train_dataloader):\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            images_weak, images_strong, bboxes, gt_masks, img_paths= data\n",
    "            del data\n",
    "\n",
    "            if iter==yu:\n",
    "                step_size = 50\n",
    "                for j in range(0, len(gt_masks[0]), step_size):\n",
    "                    \n",
    "                    \n",
    "                    gt_masks_new = gt_masks[0][j:j+step_size].unsqueeze(0)\n",
    "                    prompts = get_prompts(cfg, bboxes, gt_masks_new)\n",
    "    \n",
    "                    batch_size = images_weak.size(0)\n",
    "    \n",
    "                    entropy_maps, preds = process_forward(images_weak, prompts, model)\n",
    "                    entropy_maps = torch.stack(entropy_maps, dim=0)\n",
    "                    pred_stack = torch.stack(preds, dim=0)\n",
    "                    pred_binary = (((pred_stack)>0.7) ).float()  \n",
    "                    overlap_count = pred_binary.sum(dim=0)\n",
    "                    overlap_map = (overlap_count > 1).float()\n",
    "                    invert_overlap_map = 1.0 - overlap_map\n",
    "    \n",
    "    \n",
    "    \n",
    "                    \n",
    "    \n",
    "                    bboxes = []\n",
    "                    point_list = []\n",
    "                    point_labels_list = []\n",
    "                    for i,  pred in enumerate( preds):\n",
    "                     \n",
    "                        \n",
    "                        point_coords = prompts[0][0][i][:].unsqueeze(0)\n",
    "                        point_coords_lab = prompts[0][1][i][:].unsqueeze(0)\n",
    "    \n",
    "                        pred = (pred[0]>0.7)\n",
    "                        pred_w_overlap = pred * invert_overlap_map[0]\n",
    "    \n",
    "                        ys, xs = torch.where(pred_w_overlap > 0.5)\n",
    "                        if len(xs) > 0 and len(ys) > 0:\n",
    "                            x_min, x_max = xs.min().item(), xs.max().item()\n",
    "                            y_min, y_max = ys.min().item(), ys.max().item()\n",
    "    \n",
    "                            bboxes.append(torch.tensor([x_min, y_min , x_max, y_max], dtype=torch.float32))\n",
    "    \n",
    "                            point_list.append(point_coords)\n",
    "                            point_labels_list.append(point_coords_lab)\n",
    "                        \n",
    "                    if len(bboxes) == 0:\n",
    "                        continue  # skip if no valid region\n",
    "    \n",
    "                    point_ = torch.cat(point_list).squeeze(1)\n",
    "                    point_labels_ = torch.cat(point_labels_list)\n",
    "                    new_prompts = [(point_, point_labels_)]\n",
    "    \n",
    "                    bboxes = torch.stack(bboxes)\n",
    "    \n",
    "                    with torch.no_grad():\n",
    "                        embeddings, soft_masks, _, _ = model(images_weak, bboxes.unsqueeze(0))\n",
    "                   \n",
    "                    \n",
    "    \n",
    "                    # sof_mask_prob = torch.sigmoid(torch.stack(soft_masks, dim=0))\n",
    "                    # entropy_sm = - (sof_mask_prob * torch.log(sof_mask_prob + eps) + (1 - sof_mask_prob) * torch.log(1 - sof_mask_prob + eps))\n",
    "    \n",
    "                    # entropy_means.append(entropy_sm.detach().mean().cpu().item())\n",
    "                    \n",
    "                    _, pred_masks, iou_predictions, _= model(images_strong, prompts)\n",
    "                    del _\n",
    "    \n",
    "                    num_masks = sum(len(pred_mask) for pred_mask in pred_masks)\n",
    "                    loss_focal = torch.tensor(0., device=fabric.device)\n",
    "                    loss_dice = torch.tensor(0., device=fabric.device)\n",
    "                    loss_iou = torch.tensor(0., device=fabric.device)\n",
    "                    loss_sim = torch.tensor(0., device=fabric.device)\n",
    "    \n",
    "    \n",
    "                    batch_feats = []  # collect all bbox features in current image\n",
    "    \n",
    "                    for bbox in bboxes:\n",
    "                        feat = get_bbox_feature(embeddings, bbox)\n",
    "                        batch_feats.append(feat)\n",
    "    \n",
    "                    if len(batch_feats) > 0:\n",
    "                     \n",
    "                        batch_feats = F.normalize(torch.stack(batch_feats, dim=0), dim=1)\n",
    "                        loss_sim = similarity_loss(batch_feats, feature_queue)\n",
    "                  \n",
    "                        # add new features to queue (detach to avoid backprop)\n",
    "                        for f in batch_feats:\n",
    "                            feature_queue.append(f.detach())\n",
    "                    else:\n",
    "                        loss_sim = torch.tensor(0., device=embeddings.device)\n",
    "\n",
    "          \n",
    "\n",
    "                    visualize_instance_segmentation(images_weak, gt_masks_new, prompts, entropy_maps, pred_stack, invert_overlap_map, soft_masks, bboxes)\n",
    "    \n",
    "                   \n",
    "                 \n",
    "    \n",
    "                    for i, (pred_mask, soft_mask, iou_prediction, bbox) in enumerate(\n",
    "                            zip(pred_masks, soft_masks, iou_predictions, bboxes  )\n",
    "                        ):  \n",
    "                            embed_feats = get_bbox_feature( embeddings, bbox)\n",
    "                            embed_feats = F.normalize(embed_feats, p=2, dim=0)\n",
    "                            embedding_queue.append(embed_feats)\n",
    "    \n",
    "                            if len(embedding_queue) > -1:\n",
    "                                # Stack all embeddings (num_instances, feature_dim)\n",
    "                                features = torch.stack(embedding_queue, dim=0)  # [N, D]\n",
    "                                eps = 1e-8\n",
    "    \n",
    "                                # Compute cosine similarity matrix\n",
    "                                cos_sim_matrix = F.cosine_similarity(\n",
    "                                    features.unsqueeze(1),  # [N, 1, D]\n",
    "                                    features.unsqueeze(0),  # [1, N, D]\n",
    "                                    dim=2,\n",
    "                                    eps=eps\n",
    "                                )  # shape [N, N]\n",
    "    \n",
    "    \n",
    "    \n",
    "                                # Remove self-similarity bias\n",
    "                                num = features.size(0)\n",
    "                                mask = (1 - torch.eye(num, device=features.device))\n",
    "                                cos_sim_matrix = cos_sim_matrix * mask\n",
    "    \n",
    "                                # ---- Soft alignment (SSAL) ----\n",
    "                                # Step 1. Rescale cosine to [0,1]\n",
    "                                cos_sim_matrix = (cos_sim_matrix + 1) / 2\n",
    "    \n",
    "                                # Step 2. Compute temperature-scaled soft distribution\n",
    "                                tau = 0.05  # you can tune in [0.03â€“0.1]\n",
    "                                sim_soft = torch.exp(cos_sim_matrix / tau)\n",
    "                                prob_matrix = sim_soft / (sim_soft.sum(dim=1, keepdim=True) + eps)\n",
    "    \n",
    "                                # Step 3. Soft Semantic Alignment Loss\n",
    "                                loss_sim = ((1 - cos_sim_matrix) * prob_matrix).sum(dim=1).mean()\n",
    "    \n",
    "                            else:\n",
    "                                loss_sim = torch.tensor(0.0, device=embeddings.device)\n",
    "                           \n",
    "                            soft_mask = (soft_mask > 0.).float()\n",
    "                            # Apply entropy mask to losses\n",
    "                            loss_focal += focal_loss(pred_mask, soft_mask)  #, entropy_mask=entropy_mask\n",
    "                            loss_dice += dice_loss(pred_mask, soft_mask)   #, entropy_mask=entropy_mask\n",
    "                            batch_iou = calc_iou(pred_mask, soft_mask)\n",
    "                            loss_iou += F.mse_loss(iou_prediction, batch_iou, reduction='sum') / num_masks\n",
    "    \n",
    "                            # plt.imshow(pred_mask[0].detach().cpu().numpy(), cmap='viridis')\n",
    "                            # plt.show()\n",
    "                            # plt.imshow(soft_mask[0].detach().cpu().numpy(), cmap='viridis')\n",
    "                            # plt.show()\n",
    "                    del  pred_masks, iou_predictions \n",
    "                #     # loss_dist = loss_dist / num_masks\n",
    "                #     loss_dice = loss_dice #/ num_masks\n",
    "                #     loss_focal = loss_focal #/ num_masks\n",
    "                #     torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "                #     loss_total =  20 * loss_focal +  loss_dice  + loss_iou + 0.1*loss_sim#+ loss_iou  +  +\n",
    "    \n",
    "    \n",
    "    \n",
    "                #     fabric.backward(loss_total)\n",
    "    \n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     optimizer.zero_grad()\n",
    "                #     torch.cuda.empty_cache()\n",
    "                #     del  prompts, soft_masks\n",
    "    \n",
    "                #     batch_time.update(time.time() - end)\n",
    "                #     end = time.time()\n",
    "    \n",
    "                #     focal_losses.update(loss_focal.item(), batch_size)\n",
    "            #     dice_losses.update(loss_dice.item(), batch_size)\n",
    "            #     iou_losses.update(loss_iou.item(), batch_size)\n",
    "            #     total_losses.update(loss_total.item(), batch_size)\n",
    "            #     sim_losses.update(loss_sim.item(), batch_size)\n",
    "            \n",
    "            #     del loss_dice, loss_iou, loss_focal\n",
    "\n",
    "            # if (iter+1) % match_interval==0:\n",
    "            #     fabric.print(\n",
    "            #         f\"Epoch [{epoch}] Iter [{iter + 1}/{len(train_dataloader)}] \"\n",
    "            #         f\"| Focal {focal_losses.avg:.4f} | Dice {dice_losses.avg:.4f} | \"\n",
    "            #         f\"IoU {iou_losses.avg:.4f} | Sim_loss {sim_losses.avg:.4f} | Total {total_losses.avg:.4f}\"\n",
    "            #     )\n",
    "            # if (iter+1) % eval_interval == 0:\n",
    "            #     val_iou, _ = validate(fabric, cfg, model, val_dataloader, cfg.name, epoch)\n",
    "\n",
    "            #     status = \"\"\n",
    "            #     if val_iou > 0:  #best_iou\n",
    "            #         best_iou = val_iou\n",
    "            #         best_state = copy.deepcopy(model.state_dict())\n",
    "            #         torch.save(best_state, os.path.join(cfg.out_dir, \"save\", \"best_model.pth\"))\n",
    "            #         status = \"Improved â†’ Model Saved\"\n",
    "            #         no_improve_count = 0\n",
    "            #     else:\n",
    "            #         model.load_state_dict(best_state)\n",
    "            #         no_improve_count += 1\n",
    "            #         status = f\"Rollback ({no_improve_count})\"\n",
    "\n",
    "            #     # Write log entry\n",
    "            #     with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            #         writer = csv.writer(f)\n",
    "            #         writer.writerow([epoch, iter + 1, val_iou, best_iou, status])\n",
    "\n",
    "            #     fabric.print(f\"Validation IoU={val_iou:.4f} | Best={best_iou:.4f} | {status}\")\n",
    "\n",
    "            #     # Stop if model fails to stabilize\n",
    "            #     if no_improve_count >= max_patience:\n",
    "            #         fabric.print(f\"Training stopped early after {no_improve_count} failed rollbacks.\")\n",
    "            #         return\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f931a2af-e639-4955-8841-abcd3f6c57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Configuration Loading and Launch\n",
    "\n",
    "# %% [code]\n",
    "# Example: set arguments manually here\n",
    "# Replace with your config module path, e.g. \"configs.default_config\"\n",
    "import importlib\n",
    "\n",
    "CFG_MODULE = \"configs.config_nwpu\"\n",
    "cfg_module = importlib.import_module(CFG_MODULE)\n",
    "cfg = cfg_module.cfg\n",
    "\n",
    "# Manually merge updates if needed\n",
    "cfg.out_dir = \"./outputs\"\n",
    "cfg.resume = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c36a16-c181-4860-a180-529ea7e0c1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gpu_ids = [str(i) for i in range(torch.cuda.device_count())]\n",
    "num_devices = len(gpu_ids)\n",
    "fabric = L.Fabric(accelerator=\"auto\",\n",
    "                  devices=num_devices,\n",
    "                  strategy=\"auto\",\n",
    "                  loggers=[TensorBoardLogger(cfg.out_dir)])\n",
    "fabric.launch()\n",
    "fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    create_csv(os.path.join(cfg.out_dir, \"metrics.csv\"), csv_head=cfg.csv_keys)\n",
    "\n",
    "with fabric.device:\n",
    "    model = Model(cfg)\n",
    "    model.setup()\n",
    "\n",
    "load_datasets = call_load_dataset(cfg)\n",
    "train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt = True)\n",
    "train_data = fabric._setup_dataloader(train_data)\n",
    "val_data = fabric._setup_dataloader(val_data)\n",
    "pt_data = fabric._setup_dataloader(pt_data)\n",
    "optimizer, scheduler = configure_opt(cfg, model)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "auto_ckpt = None#\"./work_dir/nwpu/resam/point_1/save/best_model.pth\"#_find_latest_checkpoint(os.path.join(cfg.out_dir, \"save\"))\n",
    "\n",
    "print(auto_ckpt)\n",
    "if auto_ckpt is not None:\n",
    "    full_checkpoint = fabric.load(auto_ckpt)\n",
    "\n",
    "    if isinstance(full_checkpoint, dict) and \"model\" in full_checkpoint:\n",
    "        model.load_state_dict(full_checkpoint[\"model\"])\n",
    "        if \"optimizer\" in full_checkpoint:\n",
    "            optimizer.load_state_dict(full_checkpoint[\"optimizer\"])\n",
    "    else:\n",
    "        model.load_state_dict(full_checkpoint)\n",
    "    loaded = True\n",
    "    fabric.print(f\"Resumed from explicit checkpoint: {cfg.model.ckpt}\")\n",
    "\n",
    "init_iou = 0\n",
    "# print('-'*100)\n",
    "# print('\\033[92mDirect test on the original SAM.\\033[0m') \n",
    "# init_iou, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)\n",
    "# print('-'*100)\n",
    "# del _     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85837094-231d-4781-81b3-80d0561bc36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with rollback enabled. Logging to: ./outputs/training_log.csv\n",
      "374\n",
      "318\n",
      "490\n",
      "209\n",
      "130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m yu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m654\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfabric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_iou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m, in \u001b[0;36mtrain_sam\u001b[0;34m(cfg, fabric, model, optimizer, scheduler, train_dataloader, val_dataloader, init_iou, vis)\u001b[0m\n\u001b[1;32m     56\u001b[0m data \u001b[38;5;241m=\u001b[39m train_dataloader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     61\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[1;32m     62\u001b[0m     images_weak, images_strong, bboxes, gt_masks, img_paths\u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/lightning/fabric/wrappers.py:178\u001b[0m, in \u001b[0;36m_FabricDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m move_data_to_device(item, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/datasets/NWPU.py:46\u001b[0m, in \u001b[0;36mNWPUDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# image_origin = image_weak\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 46\u001b[0m     image_weak, masks_weak, bboxes_weak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_weak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_weak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes_weak\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     image_strong \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mtransform_image(image_strong)\n\u001b[1;32m     49\u001b[0m bboxes_weak \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(bboxes_weak, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/datasets/tools.py:21\u001b[0m, in \u001b[0;36mResizeAndPad.__call__\u001b[0;34m(self, image, masks, bboxes, visual)\u001b[0m\n\u001b[1;32m     19\u001b[0m og_h, og_w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mapply_image(image)\n\u001b[0;32m---> 21\u001b[0m masks \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mapply_image(mask)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks]\n\u001b[1;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(image)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Pad image and masks to form a square\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/datasets/tools.py:21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m og_h, og_w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mapply_image(image)\n\u001b[0;32m---> 21\u001b[0m masks \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks]\n\u001b[1;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(image)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Pad image and masks to form a square\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/utils/transforms.py:31\u001b[0m, in \u001b[0;36mResizeLongestSide.apply_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mExpects a numpy array with shape HxWxC in uint8 format.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m target_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_preprocess_shape(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_length)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/PIL/Image.py:2365\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2353\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2354\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2355\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2356\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2357\u001b[0m         )\n\u001b[1;32m   2358\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2359\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2360\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2361\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2362\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2363\u001b[0m         )\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yu=654\n",
    "train_sam(cfg, fabric, model, optimizer, scheduler, train_data, val_data, init_iou, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316fd58-488e-412e-9f51-ffcfd196b1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbe474-ec55-4a87-88b4-73a5fca4b126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78514ac-5be4-485a-8d15-22f936b1a92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207e5aa-4198-4126-8b0f-cd91593d4aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e85d4-5669-4b85-bf6e-b7edd7cb6880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdabbb6-0db0-4cd1-88e3-35c6c0c48115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71d266-7a26-4ca3-aa4b-23dfb6f981da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa6aa6-689a-4b8f-a871-e71e43f902ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f68291-6ae9-4e9a-be37-75ebbd44f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0631ab-d878-48b3-9374-7249a22b96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac6d54-a0d9-4a48-8bdf-d08929517f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb978-f670-4a17-83e5-5cb1743478f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65bd4a-32e5-4bed-ab13-d5daba4c818a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8515677-fc44-47cd-b80b-a8ec8da98bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6e1a4-14aa-4237-8036-a0db0cbfffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e405fd-4f46-479b-9967-f60c7f14150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf444b-577e-4699-84b1-f6855ab29993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b5c87-d442-4cd8-b9b6-a893fde2a9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c745d-2514-4b97-a00d-b7e3f6fda48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadb49b-35cc-449b-bc73-19dcc98b1737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb33455-e207-4330-b17c-4b962df61bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82e71d-29e7-43c2-8adc-043c97840757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e85597-5593-4cc5-9bef-08fbce3b402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter, data in enumerate(train_data):\n",
    "    images_weak, images_strong, bboxes, gt_masks, img_paths= data\n",
    "    print(bboxes)\n",
    "    if iter>2:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81265e2c-95cf-4108-86ba-516022e5af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70e5a2-a8d3-436f-ae8f-632a8866bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_calibration(cfg, entrop_map, prompts, point_status):\n",
    "    point_list = []\n",
    "    point_labels_list = []\n",
    "    num_points = cfg.num_points\n",
    "\n",
    "    for m in range(len(entrop_map)):\n",
    "        point_coords = prompts[0][0][m][:].unsqueeze(0)\n",
    "        point_coords_lab = prompts[0][1][m][:].unsqueeze(0)\n",
    "\n",
    "        # Find high-entropy location\n",
    "        max_idx = torch.argmax(entrop_map[m])\n",
    "        y = max_idx // entrop_map[m].shape[1]\n",
    "        x = max_idx % entrop_map[m].shape[1]\n",
    "        neg_point_coords = torch.tensor([[x.item(), y.item()]], device=point_coords.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "        # Combine positive and negative points\n",
    "        point_coords_all = torch.cat((point_coords, neg_point_coords), dim=1)\n",
    "        \n",
    "        # Append a new label (1) to the label tensor\n",
    "        point_labels_all = torch.cat(\n",
    "            (point_coords_lab, torch.tensor([[point_status]], device=point_coords.device, dtype=point_coords_lab.dtype)),\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        point_list.append(point_coords_all)\n",
    "        point_labels_list.append(point_labels_all)\n",
    "\n",
    "\n",
    "\n",
    "    point_ = torch.cat(point_list).squeeze(1)\n",
    "    point_labels_ = torch.cat(point_labels_list)\n",
    "    new_prompts = [(point_, point_labels_)]\n",
    "    return new_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79477525-9853-4ad9-a975-e72a582627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def entropy_map_calculate(p, eps=1e-8):\n",
    "    # Clamp to avoid log(0)\n",
    "    p = torch.clamp(p, eps, 1 - eps)\n",
    "    \n",
    "    # Compute binary entropy\n",
    "    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    \n",
    "    # Normalize to 0â€“1 (since max entropy = log(2))\n",
    "    entropy_map = entropy_map / torch.log(torch.tensor(2.0))\n",
    "    \n",
    "    return entropy_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c39a90-63d8-45fa-8876-654e1a5410a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forward(img_tensor, prompt):\n",
    "    with torch.no_grad():\n",
    "        _, masks_pred, _, _ = model(img_tensor, prompt)\n",
    "    entropy_maps = []\n",
    "    pred_ins = []\n",
    "    for i, mask_p in enumerate( masks_pred[0]):\n",
    "\n",
    "        p = mask_p.clamp(1e-6, 1 - 1e-6)\n",
    "        if p.ndim == 2:\n",
    "            p = p.unsqueeze(0)\n",
    "\n",
    "        entropy_map = entropy_map_calculate(p)\n",
    "        entropy_maps.append(entropy_map)\n",
    "        pred_ins.append(p)\n",
    "\n",
    "    return entropy_maps, pred_ins\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c592e9c-6861-4a20-a5ac-e39ae2db12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def edge_corner_score(x, y, x_c, y_c, w, h, gamma=1.0):\n",
    "    dx = 2 * torch.abs(x - x_c) / w\n",
    "    dy = 2 * torch.abs(y - y_c) / h\n",
    "    dx = torch.clamp(dx, 0, 1)\n",
    "    dy = torch.clamp(dy, 0, 1)\n",
    "    score = (dx + dy - dx * dy) ** gamma\n",
    "    return score\n",
    "\n",
    "# Rectangle parameters\n",
    "w, h = 100, 500\n",
    "x_c, y_c = w / 2, h / 2\n",
    "\n",
    "# Create grid of coordinates\n",
    "x = torch.linspace(0, w, 400)\n",
    "y = torch.linspace(0, h, 300)\n",
    "xx, yy = torch.meshgrid(x, y, indexing=\"xy\")\n",
    "\n",
    "# Compute edge/corner score over the grid\n",
    "score_map = edge_corner_score(xx, yy, x_c, y_c, w, h, gamma=0.7)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(score_map.numpy(), origin=\"lower\", cmap=\"plasma\", extent=[0, w, 0, h])\n",
    "plt.colorbar(label=\"Edge/Corner Score\")\n",
    "plt.title(\"Edge + Corner Proximity Heatmap\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter([x_c], [y_c], c=\"white\", s=30, label=\"Center\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df225794-7276-4b4a-9248-963f315c29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edge_corner_score(x, y, x_c, y_c, w, h, gamma=0.7):\n",
    "    dx = 2 * torch.abs(x - x_c) / w\n",
    "    dy = 2 * torch.abs(y - y_c) / h\n",
    "    dx = torch.clamp(dx, 0, 1)\n",
    "    dy = torch.clamp(dy, 0, 1)\n",
    "    # high on edges + corners, low at center\n",
    "    score = (dx + dy - dx * dy) ** gamma\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3bfc-2d18-4c57-bc0f-cb0f4ed6e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def adjust_bbox_by_gradient(prob_map, bbox, max_iter=50, step=1, gain_thresh=0.001, device='cpu'):\n",
    "    \"\"\"\n",
    "    Expands or shifts a bounding box toward the segmentation region\n",
    "    guided by gradients in the probability map.\n",
    "    \"\"\"\n",
    "    if isinstance(prob_map, np.ndarray):\n",
    "        prob_map = torch.from_numpy(prob_map)\n",
    "    prob_map = prob_map.float().to(device)\n",
    "    H, W = prob_map.shape\n",
    "\n",
    "    bbox = torch.tensor(bbox, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute gradients once\n",
    "    gy, gx = torch.gradient(prob_map)\n",
    "    grad_mag = torch.sqrt(gx**2 + gy**2)\n",
    "\n",
    "    def clamp_box(b):\n",
    "        x1, y1, x2, y2 = b\n",
    "        return torch.tensor([\n",
    "            torch.clamp(x1, 0, W-1),\n",
    "            torch.clamp(y1, 0, H-1),\n",
    "            torch.clamp(x2, 1, W),\n",
    "            torch.clamp(y2, 1, H)\n",
    "        ], device=device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x1, y1, x2, y2 = bbox.int()\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(W-1, x2), min(H-1, y2)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            break\n",
    "\n",
    "        # Measure edge gradient means\n",
    "        top_grad = grad_mag[y1, x1:x2].mean()\n",
    "        bottom_grad = grad_mag[y2-1, x1:x2].mean()\n",
    "        left_grad = grad_mag[y1:y2, x1].mean()\n",
    "        right_grad = grad_mag[y1:y2, x2-1].mean()\n",
    "\n",
    "        edges = {\n",
    "            'up': top_grad,\n",
    "            'down': bottom_grad,\n",
    "            'left': left_grad,\n",
    "            'right': right_grad\n",
    "        }\n",
    "\n",
    "        best_edge = max(edges, key=edges.get)\n",
    "        best_grad = edges[best_edge].item()\n",
    "\n",
    "        if best_grad < gain_thresh:\n",
    "            break\n",
    "\n",
    "        # Move or expand the chosen edge outward\n",
    "        if best_edge == 'up':\n",
    "            bbox[1] -= step\n",
    "        elif best_edge == 'down':\n",
    "            bbox[3] += step\n",
    "        elif best_edge == 'left':\n",
    "            bbox[0] -= step\n",
    "        elif best_edge == 'right':\n",
    "            bbox[2] += step\n",
    "\n",
    "        bbox = clamp_box(bbox)\n",
    "\n",
    "    # Final mask\n",
    "    final_mask = torch.zeros_like(prob_map)\n",
    "    x1, y1, x2, y2 = bbox.int()\n",
    "    final_mask[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "    return bbox.cpu().tolist(), final_mask.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13625b-f65c-48ec-887c-275744a4bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "save_dir = \"entropy_sorted\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "W, H = 1024, 1024\n",
    "\n",
    "for rank, (entropy_scalar, img_path, render) in enumerate(collected, start=1):\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # ---- Convert and move to device\n",
    "    img_np = render['real']        # numpy HxWx3\n",
    "    img_tensor = torch.from_numpy(img_np).permute(2,0,1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "    prompt_main = render['prompt']\n",
    "    # ---- Forward pass to get entropy + predictions\n",
    "    # entropy_maps, preds = process_forward(img_tensor, prompt_main)\n",
    "\n",
    "    # print(prompt_main[0][0].shape)\n",
    "    # pred_stack = torch.stack(preds, dim=0)\n",
    "\n",
    "    # # Convert to binary masks (e.g., threshold 0.99 as you do)\n",
    "    # pred_binary = (pred_stack > 0.99).float() \n",
    "    # # Count overlaps\n",
    "    # overlap_count = pred_binary.sum(dim=0)  # (1,1024,1024)\n",
    "    \n",
    "    # # Optional: extract overlapping region mask (for debugging)\n",
    "    # overlap_map = (overlap_count > 1).float()\n",
    "    # invert_overlap_map = 1.0 - overlap_map\n",
    "    \n",
    "    # # Convert to uint8 for visualization/saving\n",
    "    # # non_overlap_vis = (non_overlap_map[0].cpu().numpy() * 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    # # # ---- Save the original image once\n",
    "    img_save_path = os.path.join(save_dir, f\"{rank}.jpg\")\n",
    "    cv2.imwrite(img_save_path, cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # scores = []\n",
    "    # # ---- Loop over instances (each mask + entropy)\n",
    "    # for i, (entr_map, pred) in enumerate(zip(entropy_maps, preds)):\n",
    "    #     # # Normalize entropy 0â€“1\n",
    "    #     # entr_norm = (entr_map - entr_map.min()) / (entr_map.max() - entr_map.min() + 1e-8)\n",
    "\n",
    "    #     # # Convert to uint8\n",
    "    #     # entr_vis = (entr_norm[0].cpu().numpy() * 255).astype(np.uint8)\n",
    "    #     pred = (pred[0]>0.99) \n",
    "    #     pred_w_overlap = pred * invert_overlap_map[0]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    #     # Find where mask == 1\n",
    "    #     ys, xs = torch.where(pred_w_overlap > 0.5)\n",
    "    #     if len(xs) > 0 and len(ys) > 0:\n",
    "    #         x_min, x_max = xs.min().item(), xs.max().item()\n",
    "    #         y_min, y_max = ys.min().item(), ys.max().item()\n",
    "    #         h, w = y_max - y_min, x_max - x_min\n",
    "    #         print(f\"Bounding box: ({x_min}, {y_min}) â†’ ({x_max}, {y_max})\") \n",
    "    #         cx = (x_min + x_max) / 2.0\n",
    "    #         cy = (y_min + y_max) / 2.0\n",
    "    #         # # Optional: draw rectangle\n",
    "    #         # mask_vis = (pred_w_overlap.cpu().numpy() * 255).astype(np.uint8)\n",
    "    #         # mask_rgb = cv2.cvtColor(mask_vis, cv2.COLOR_GRAY2BGR)\n",
    "    #         # cv2.rectangle(mask_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    #         # pred_path = os.path.join(save_dir, f\"{rank:04d}_{img_name}_{i}mask_with_box.png\")\n",
    "    #         # cv2.imwrite(pred_path, mask_rgb)\n",
    "    #     else:\n",
    "    #         print(\"No 1s found in mask\")\n",
    "    #     # Make sure both tensors are on same device\n",
    "    #     point_ref = torch.tensor([cx, cy], dtype=torch.float32, device=prompt_main[0][0][i].device)\n",
    "    #     score = edge_corner_score(prompt_main[0][0][i][0][0].item(), prompt_main[0][0][i][0][1].item(), point_ref[0], point_ref[1], w, h)\n",
    "    #     scores.append(score)\n",
    "    #     # point_ref = torch.tensor([cx, cy], dtype=torch.float32, device=prompt_main[0][0][i].device)\n",
    "    #     # dist = torch.dist(prompt_main[0][0][i], point_ref)\n",
    "        \n",
    "    #     # pred_w_overlap_vis = (pred_w_overlap.cpu().numpy()* 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    #     # # Save prediction mask\n",
    "    #     # pred_path = os.path.join(save_dir, f\"{rank:04d}_{img_name}_{i}_pred.png\")\n",
    "    #     # cv2.imwrite(pred_path, pred_w_overlap_vis)\n",
    "    # print(f\"Saved image + {len(entropy_maps)} instance maps for {img_name}\")\n",
    "\n",
    "\n",
    "    x_offset = 16\n",
    "    y_offset = 16\n",
    "    refine_iter = 20\n",
    "    for refine in range(refine_iter):\n",
    "\n",
    "        if refine == 0:\n",
    "            bboxes = []\n",
    "            for j in range(len(prompt_main[0][0])):\n",
    "                x = prompt_main[0][0][j][0][0]\n",
    "                y = prompt_main[0][0][j][0][1]\n",
    "    \n",
    "                bboxes.append(torch.tensor([x-x_offset, y - y_offset, x + x_offset, y + y_offset], dtype=torch.float32))\n",
    "\n",
    "            bboxes = torch.stack(bboxes)\n",
    "        with torch.no_grad():\n",
    "            _, masks_pred, _, _ = model(img_tensor, bboxes.unsqueeze(0))\n",
    "\n",
    "        # masks_pred[0] shape -> [num_boxes, H, W]\n",
    "        pred_masks = masks_pred[0].cpu()  \n",
    "        bbox_new = []\n",
    "        for i, mask in enumerate(pred_masks):\n",
    "            # Threshold mask to binary: 0 or 255\n",
    "            mask_bin = (mask > 0.5).numpy().astype(np.uint8) * 255  # clean mask\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # -------------------------\n",
    "            # Compute and save entropy heatmap\n",
    "            # -------------------------\n",
    "            # Convert mask to probability (0-1)\n",
    "            mask_prob = mask.numpy()\n",
    "            \n",
    "            # Avoid log(0) by clamping probabilities\n",
    "            eps = 1e-6\n",
    "            mask_prob = np.clip(mask_prob, eps, 1 - eps)\n",
    "            # Compute entropy per pixel\n",
    "            entropy_map = - (mask_prob * np.log(mask_prob) + (1 - mask_prob) * np.log(1 - mask_prob))\n",
    "            \n",
    "            # Normalize entropy to 0-255 for visualization\n",
    "            entropy_norm = ((entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min()) * 255).astype(np.uint8)\n",
    "            \n",
    "            # Crop the entropy map to the bounding box\n",
    "            entropy_roi = entropy_map[y1:y2, x1:x2]\n",
    "            \n",
    "            # Compute mean entropy inside the rectangle\n",
    "            mean_entropy = entropy_roi.mean()\n",
    "            \n",
    "            print(f\"Mask {i} mean entropy inside bbox: {mean_entropy:.4f}\")\n",
    "                    \n",
    "\n",
    "      \n",
    "            prob_map = mask.numpy()\n",
    "            opt_box, _ = adjust_bbox_by_gradient(prob_map, bboxes[i].int().tolist())\n",
    "            opt_box = list(map(int, opt_box))\n",
    "          \n",
    "            bbox_new.append(torch.tensor(opt_box, dtype=torch.float32))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            if refine == (refine_iter-1):\n",
    "                \n",
    "                # Convert to 3-channel for colored box\n",
    "                mask_color = cv2.cvtColor(mask_bin, cv2.COLOR_GRAY2BGR)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = bboxes[i].int().tolist()\n",
    "                cv2.rectangle(mask_color, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "                # Apply a colormap for better visualization\n",
    "                entropy_color = cv2.applyColorMap(entropy_norm, cv2.COLORMAP_JET)\n",
    "                # Draw bounding box on entropy map as well\n",
    "                cv2.rectangle(entropy_color, (x1, y1), (x2, y2), color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                # Save entropy heatmap\n",
    "                save_path_entropy = os.path.join(save_dir, f\"{rank}_{i}_entropy_heatmap.png\")\n",
    "                cv2.imwrite(save_path_entropy, entropy_color)\n",
    "                print(f\"Saved {save_path_entropy}\")\n",
    "    \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = opt_box\n",
    "                cv2.rectangle(mask_color, (x1, y1), (x2, y2), color=(0, 255, 255), thickness=2)\n",
    "                \n",
    "                \n",
    "    \n",
    "                # Save the clean mask with box\n",
    "                save_path = os.path.join(save_dir, f\"{rank}_{i}_mask_clean_box.png\")\n",
    "                cv2.imwrite(save_path, mask_color)\n",
    "                print(f\"Saved {save_path}\")\n",
    "        \n",
    "        bbox_new = torch.stack(bbox_new)\n",
    "        bboxes = bbox_new\n",
    "     \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # Stop early for testing\n",
    "    if rank >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8152bb-c4b6-49fe-bc8c-5f7c0d7b67da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e63697-036b-4adc-b308-7032bd381113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66827a0-c7b6-477e-ba19-eaa1dcf4058c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f221a73-a518-4ca1-8937-97dc22ec4ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13c127-fbb0-4fa3-b97f-a3136113262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a3fa9-187d-49b7-a82d-fcc9b3dfa523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d738313-05d0-4e83-b730-f25aeacb3a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f268a-ac20-4137-ae18-07364c9505f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86362cd-940f-4751-a35b-1874690203da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9e8bf-296b-41a3-9144-1ba16412490c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c57b8-6618-4417-b0e9-affcec932b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823b846-cf4f-4780-bff5-a1ee4704edbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0407f-15d1-4e5e-96e2-c2e6137b6e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202aadd-e917-4b44-8e2a-ef7dfd8ec566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db847e9f-7d77-4665-8a70-0056cc0d8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb27bb-27ee-4525-b4aa-ca09627e7eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5a99e-b911-45e9-afcc-a6a311b1f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d104352-115c-4c57-9c0f-c046be931f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d60d9-553a-4450-8ffe-b378b9ea517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc3acd-ff22-4fbe-b1de-36a05ec539f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b7147-d578-4edf-88fe-0daea9a9e2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c22afc-3053-4eee-86fd-78b2f31ce6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06310b-0091-476f-ad9c-980799369adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c9f30-d4b9-4d36-93fd-b38c6433b0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65916a-16ec-47ac-9187-1043cd38ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff142d69-d26f-4f37-9ecb-edf7c8e2b7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ddeec-5305-40ed-a314-4a4237f1ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db350-c3be-43e0-a01d-09fdb95f13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b9cdb-bafc-4b3b-bd04-d4f9157d62a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4b887-6a7a-4d33-a252-909c5a45c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb382e73-510f-440f-9522-96d091306df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82482b0a-98f5-4a2b-9033-9b211534dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237be62-e469-4655-8822-a0be202fa5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01301e45-b4d2-4615-b3f0-0be198502a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8e4fb-5aa2-45e9-af88-ca0651d157c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
