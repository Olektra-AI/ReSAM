

import os
import time
import argparse
import random
# from abc import ABC

import cv2
import numpy as np
import torch
# import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from PIL import Image
from matplotlib import cm

from scipy.ndimage import label
import numpy as np

import lightning as L
from lightning.fabric.loggers import TensorBoardLogger
from lightning.fabric.fabric import _FabricOptimizer

from box import Box
from datasets import call_load_dataset
from utils.model import Model
from utils.losses import DiceLoss, FocalLoss, Matching_Loss, cosine_similarity
from utils.eval_utils import AverageMeter, validate, get_prompts, calc_iou, validate_sam2
from utils.tools import copy_model, create_csv, reduce_instances
from utils.utils import *

import  csv, copy
import torch
import torch.nn.functional as F
from collections import deque

# vis = False


class LossWatcher:
    def __init__(self, window=100, factor=10.0):
        self.window = window
        self.factor = factor
        self.losses = []
    
    def is_outlier(self, loss):
        if not torch.isfinite(loss):
            return True
        self.losses.append(loss.item())
        if len(self.losses) < self.window:
            return False
        recent_avg = sum(self.losses[-self.window:]) / self.window
        return loss.item() > recent_avg * self.factor

def _find_latest_checkpoint(save_dir):
    """
    Look for the most recent .pt/.pth file in save_dir.
    Returns absolute path or None if not found.
    """
    if not os.path.isdir(save_dir):
        return None
    ckpt_files = [
        os.path.join(save_dir, f)
        for f in os.listdir(save_dir)
        if f.endswith(".pt") or f.endswith(".pth")
    ]
    if not ckpt_files:
        return None
    ckpt_files.sort(key=lambda p: os.path.getmtime(p), reverse=True)
    return ckpt_files[0]

def sort_entropy_(model, target_pts):

    # save_dir = "entropy_sorted"
    # os.makedirs(save_dir, exist_ok=True)

    collected = []
    with torch.no_grad():
        for i, batch in enumerate(tqdm(target_pts, desc='Computing per-sample entropy', ncols=100)):
            imgs, boxes, masks, img_paths = batch
            prompts = get_prompts(cfg, boxes, masks)
            embeds, masks_pred, _, _ = model(imgs, prompts)

            batch_size = imgs.shape[0]
            for b in range(batch_size):
                img_np = (imgs[b].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)
                p_b = masks_pred[b].clamp(1e-6, 1 - 1e-6)
                if p_b.ndim == 2:
                    p_b = p_b.unsqueeze(0)
                gt_b = masks[b]
                if gt_b.ndim == 2:
                    gt_b = gt_b.unsqueeze(0)

                entropy_scalar = 0
                num_inst = p_b.shape[0]
                for j in range(num_inst):
                    p_inst = p_b[j]
                    entropy_map_inst = - (p_inst * torch.log(p_inst) + (1 - p_inst) * torch.log(1 - p_inst))
                    entropy_scalar += float(entropy_map_inst.mean().cpu().item())

                entropy_scalar /= num_inst
                render = {
                    'real': img_np,
                    'prompt': prompts
                }
                img_path = img_paths[b] if isinstance(img_paths, (list, tuple)) else img_paths
                collected.append((entropy_scalar, img_path, render))

            if i>10:
                break

    collected.sort(key=lambda x: x[0], reverse=True)

    return collected


def get_bbox_feature(embedding_map, bbox, stride=16, pooling='avg'):
    """
    Extract a feature vector from an embedding map given a bounding box.
    
    Args:
        embedding_map (torch.Tensor): Shape (C, H_feat, W_feat) or (B, C, H_feat, W_feat)
        bbox (list or torch.Tensor): [x1, y1, x2, y2] in original image coordinates
        stride (int): Downscaling factor between image and feature map
        pooling (str): 'avg' or 'max' pooling inside the bbox region
        
    Returns:
        torch.Tensor: Feature vector of shape (C,)
    """
    # If batch dimension exists, assume batch size 1
    if embedding_map.dim() == 4:
        embedding_map = embedding_map[0]

    C, H_feat, W_feat = embedding_map.shape
    x1, y1, x2, y2 = bbox

    # Map bbox to feature map coordinates
    fx1 = max(int(x1 / stride), 0)
    fy1 = max(int(y1 / stride), 0)
    fx2 = min(int((x2 + stride - 1) / stride), W_feat)  # ceil division
    fy2 = min(int((y2 + stride - 1) / stride), H_feat)

    # Crop the feature map to bbox region
    region = embedding_map[:, fy1:fy2, fx1:fx2]

    if region.numel() == 0:
        # fallback to global feature if bbox is too small
        region = embedding_map

    # Pool to get a single feature vector
    if pooling == 'avg':
        feature_vec = region.mean(dim=(1,2))
    elif pooling == 'max':
        feature_vec = region.amax(dim=(1,2))
    else:
        raise ValueError("pooling must be 'avg' or 'max'")

    return feature_vec




def create_entropy_mask(entropy_maps, threshold=0.5, device='cuda'):
    """
    Create a mask to reduce learning from high entropy regions.
    
    Args:
        entropy_maps: List of entropy maps for each instance
        threshold: Entropy threshold above which to mask out regions
        device: Device to place the mask on
    
    Returns:
        List of entropy masks (0 for high entropy, 1 for low entropy)
    """
    entropy_masks = []
    
    for entropy_map in entropy_maps:
        # Create binary mask: 1 for low entropy, 0 for high entropy
        entropy_mask = (entropy_map < threshold).float()
        entropy_masks.append(entropy_mask)
    
    return entropy_masks


# def process_forward(img_tensor, prompt, model):
#     with torch.no_grad():
#         _, masks_pred, _, _ = model(img_tensor, prompt)
#     entropy_maps = []
#     pred_ins = []
#     eps=1e-8
#     for i, mask_p in enumerate( masks_pred[0]):
#         mask_p = torch.sigmoid(mask_p)
#         p = mask_p.clamp(1e-6, 1 - 1e-6)
#         if p.ndim == 2:
#             p = p.unsqueeze(0)

#         # entropy_map = entropy_map_calculate(p)
#         entropy = - (p * torch.log(p + eps) + (1 - p) * torch.log(1 - p + eps))
#         max_ent = torch.log(torch.tensor(2.0, device=mask_p.device))
#         entropy_norm = entropy / (max_ent + 1e-8)   # [0, 1]
#         entropy_maps.append(entropy_norm)
#         pred_ins.append(p)

#     return entropy_maps, pred_ins

def process_forward(img_tensor, prompt, model):
    with torch.no_grad():
        _, masks_pred, _, _ = model(img_tensor, prompt)
    entropy_maps = []
    pred_ins = []
    eps=1e-8
    for i, mask_p in enumerate( masks_pred[0]):
        mask_p = torch.sigmoid(mask_p)
        p = mask_p.clamp(1e-6, 1 - 1e-6)
        if p.ndim == 2:
            p = p.unsqueeze(0)

        entropy_map = entropy_map_calculate(p)
        entropy = - (p * torch.log(p + eps) + (1 - p) * torch.log(1 - p + eps))
        # max_ent = torch.log(torch.tensor(2.0, device=mask_p.device))
        # entropy_norm = entropy / (max_ent + 1e-8)   # [0, 1]
        entropy_maps.append(entropy)
        pred_ins.append(p)

    # P = torch.stack(pred_ins, dim=0)
    # P_sum = P.sum(dim=0, keepdim=True) + eps     # (1, H, W)
    # P_norm = P / P_sum 

    # # Compute entropy
    # entropy = - (P_norm * torch.log(P_norm + eps)).sum(dim=0)  # (H, W)

    # # Normalize entropy to [0, 1]
    # max_ent = torch.log(torch.tensor(P.shape[0], device=P.device).float())
    # entropy_norm = entropy / (max_ent + eps)


    return entropy_maps, pred_ins
        
        
        
        

def edge_corner_score(x, y, x_c, y_c, w, h, gamma=0.7):
    dx = 2 * torch.abs(x - x_c) / w
    dy = 2 * torch.abs(y - y_c) / h
    dx = torch.clamp(dx, 0, 1)
    dy = torch.clamp(dy, 0, 1)
    # high on edges + corners, low at center
    score = (dx + dy - dx * dy) ** gamma
    return score

import torch
import torch.nn.functional as F
from collections import deque

# persistent feature queue
feature_queue = deque(maxlen=32)  # keep up to 512 previous object embeddings

# def similarity_loss(g,features, queue, tau=0.07):
#     """
#     features: [B, D] current batch embeddings (normalized)
#     queue: deque of [D] past embeddings (detached)
#     """
#     if len(queue) == 0:
#         return torch.tensor(0., device=g.device)

#     # Stack all past features from queue
#     with torch.no_grad():
#         past_feats = torch.stack(list(queue), dim=0)  # [Q, D]
#         features = torch.stack(list(features), dim=0)  # [Q, D]

#     # Normalize
#     features = F.normalize(features, dim=1)
#     past_feats = F.normalize(past_feats, dim=1)

#     # Compute cosine similarities (batch x queue)
#     logits = torch.mm(features, past_feats.t()) / tau  # [B, Q]
#     probs = F.softmax(logits, dim=1)

#     # Weighted alignment (like SSAL)
#     cos = (logits * tau).clamp(-1, 1)  # revert scaling, approximate cos
#     loss = ((1 - cos) * probs).sum(dim=1).mean()

#     return loss

def similarity_loss(features, queue, tau=0.07, sim_threshold=0.5):
    """
    features: [B, D] current batch embeddings (normalized)
    queue: deque of [D] past embeddings (detached)
    tau: temperature for softmax
    sim_threshold: cosine similarity threshold to consider "similar"
    """
    if len(queue) == 0:
        return -1

    # Stack past features from queue
    past_feats = torch.stack(list(queue), dim=0)  # [Q, D]
    features = torch.stack(list(features), dim=0)  # [B, D]

    # Normalize embeddings
    features = F.normalize(features, dim=1)
    past_feats = F.normalize(past_feats, dim=1)

    # Compute cosine similarities
    cos_sim = torch.mm(features, past_feats.t())  # [B, Q]

    # Apply threshold: set values below threshold to 0
    mask = (cos_sim >= sim_threshold).float()
    cos_sim_masked = cos_sim * mask  # [B, Q], below threshold becomes 0

    # Scale by temperature
    logits = cos_sim_masked / tau

    # Softmax over queue dimension
    probs = F.softmax(logits, dim=1)

    # Weighted alignment loss
    loss = ((1 - cos_sim_masked) * probs).sum(dim=1).mean()

    return loss
        
def entropy_map_calculate(p):
    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))
    entropy_map = entropy_map.max(dim=0)[0]

    return entropy_map

def prompt_calibration(cfg, entrop_map, prompts, point_status):
    point_list = []
    point_labels_list = []
    num_points = cfg.num_points

    for m in range(len(entrop_map)):
        point_coords = prompts[0][0][m][:].unsqueeze(0)
        point_coords_lab = prompts[0][1][m][:].unsqueeze(0)

        # Find high-entropy location
        max_idx = torch.argmax(entrop_map[m])
        y = max_idx // entrop_map[m].shape[1]
        x = max_idx % entrop_map[m].shape[1]
        neg_point_coords = torch.tensor([[x.item(), y.item()]], device=point_coords.device).unsqueeze(0)


        # Combine positive and negative points
        point_coords_all = torch.cat((point_coords, neg_point_coords), dim=1)
        
        # Append a new label (1) to the label tensor
        point_labels_all = torch.cat(
            (point_coords_lab, torch.tensor([[point_status]], device=point_coords.device, dtype=point_coords_lab.dtype)),
            dim=1
        )
        
        point_list.append(point_coords_all)
        point_labels_list.append(point_labels_all)





    point_ = torch.cat(point_list).squeeze(1)
    point_labels_ = torch.cat(point_labels_list)
    new_prompts = [(point_, point_labels_)]
    return new_prompts



def train_sam(
    cfg: Box,
    fabric: L.Fabric,
    model: Model,
    optimizer: _FabricOptimizer,
    scheduler: _FabricOptimizer,
    train_dataloader: DataLoader,
    val_dataloader: DataLoader,
    init_iou,
):

    watcher = LossWatcher(window=50, factor=4)
    # collected = sort_entropy_(model, target_pts)
    focal_loss = FocalLoss()
    dice_loss = DiceLoss()
    best_ent = init_iou
    best_state = copy.deepcopy(model.state_dict())
    no_improve_count = 0
    max_patience = cfg.get("patience", 3)  # stop if no improvement for X validations
    match_interval = cfg.match_interval
    eval_interval = int(len(train_dataloader) * 1)

    window_size = 30

    embedding_queue = []
    ite_em = 0

    # Prepare output dirs
    os.makedirs(os.path.join(cfg.out_dir, "save"), exist_ok=True)
    csv_path = os.path.join(cfg.out_dir, "training_log.csv")

    # Initialize CSV
    with open(csv_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Epoch", "Iteration", "Val_ent", "Best_ent", "Status"])

    fabric.print(f"Training with rollback enabled. Logging to: {csv_path}")

    entropy_means = deque(maxlen=len(train_dataloader))

    # overlap_ratios = []

    eps = 1e-8
    for epoch in range(1, cfg.num_epochs + 1):
        batch_time = AverageMeter()
        data_time = AverageMeter()
        focal_losses = AverageMeter()
        dice_losses = AverageMeter()
        iou_losses = AverageMeter()
        total_losses = AverageMeter()
        match_losses = AverageMeter()
        end = time.time()
        sim_losses = AverageMeter()
        num_iter = len(train_dataloader)
        entropy_means.clear()



        for iter, data in enumerate(train_dataloader):
            
            data_time.update(time.time() - end)
            images_weak, images_strong, bboxes, gt_masks, img_paths= data
            del data

            
            step_size = 50
            for j in range(0, len(gt_masks[0]), step_size):
                
                
                gt_masks_new = gt_masks[0][j:j+step_size].unsqueeze(0)
                prompts = get_prompts(cfg, bboxes, gt_masks_new)

                batch_size = images_weak.size(0)

                entropy_maps, preds = process_forward(images_weak, prompts, model)
                
                pred_stack = torch.stack(preds, dim=0)
                entropy_maps = torch.stack(entropy_maps, dim=0)

                # mean_thresh = pred_stack[pred_stack > 0.5].mean()
                # mean_thresh = 0.7
                # pred_binary = (((pred_stack) ) > 0.7).float()
                pred_binary = ((entropy_maps < 0.4) & (pred_stack > 0.5) ).float()
                overlap_count = pred_binary.sum(dim=0)
                overlap_map = (overlap_count > 1).float()
                invert_overlap_map = 1.0 - overlap_map

                


                bboxes = []
                point_list = []
                point_labels_list = []
                for i,  (pred, ent) in enumerate( zip(pred_binary, entropy_maps)):
                    point_coords = prompts[0][0][i][:].unsqueeze(0)
                    point_coords_lab = prompts[0][1][i][:].unsqueeze(0)

                    pred_w_overlap = ((pred[0]*invert_overlap_map[0]  ) )#    * ((1 - 0.1 * ent[0]))
                    ys, xs = torch.where(pred_w_overlap > 0.5)
                    if len(xs) > 0 and len(ys) > 0:
                        x_min, x_max = xs.min().item(), xs.max().item()
                        y_min, y_max = ys.min().item(), ys.max().item()

                        bboxes.append(torch.tensor([x_min, y_min , x_max, y_max], dtype=torch.float32))

                        point_list.append(point_coords)
                        point_labels_list.append(point_coords_lab)
                    
                if len(bboxes) == 0:
                    continue  # skip if no valid region

                point_ = torch.cat(point_list).squeeze(1)
                point_labels_ = torch.cat(point_labels_list)
                new_prompts = [(point_, point_labels_)]

                bboxes = torch.stack(bboxes)

                with torch.no_grad():
                    embeddings, soft_masks, _, _ = model(images_weak, bboxes.unsqueeze(0))

                sof_mask_prob = torch.sigmoid(torch.stack(soft_masks, dim=0))
                entropy_sm = - (sof_mask_prob * torch.log(sof_mask_prob + eps) + (1 - sof_mask_prob) * torch.log(1 - sof_mask_prob + eps))

                entropy_means.append(entropy_sm.detach().mean().cpu().item())


                _, pred_masks, iou_predictions, _= model(images_strong, prompts)
                del _

                num_masks = sum(len(pred_mask) for pred_mask in pred_masks)
                loss_focal = torch.tensor(0., device=fabric.device)
                loss_dice = torch.tensor(0., device=fabric.device)
                loss_iou = torch.tensor(0., device=fabric.device)
                loss_sim = torch.tensor(0., device=fabric.device)


                batch_feats = []  # collect all bbox features in current image



                for bbox in bboxes:
                    feat = get_bbox_feature(embeddings, bbox)
                    batch_feats.append(feat)

                if len(batch_feats) > 0:
                 
                    batch_feats = F.normalize(torch.stack(batch_feats, dim=0), dim=1)
                    loss_sim = similarity_loss(feature_queue , feature_queue)

                    if loss_sim == -1:
                        loss_sim = torch.tensor(0., device=batch_feats.device)
              
                    # add new features to queue (detach to avoid backprop)
                    for f in batch_feats:
                        feature_queue.append(f.detach())
                else:
                    loss_sim = torch.tensor(0., device=embeddings.device)

        

                for i, (pred_mask, soft_mask, iou_prediction, bbox) in enumerate(
                        zip(pred_masks[0], soft_masks[0], iou_predictions[0], bboxes  )
                    ):
                        soft_mask = (soft_mask > 0.).float()
                        # print(overlap_map.shape, pred_mask.shape, soft_mask.shape)
                        # pred_mask = pred_mask * invert_overlap_map[0]
                        # soft_mask = soft_mask * invert_overlap_map[0]
                        
                        # plt.imshow(pred_mask.detach().cpu().numpy(), cmap='viridis')
                        # plt.show()
                        # plt.imshow(soft_mask.detach().cpu().numpy(), cmap='viridis')
                        # plt.show()
                        # Apply entropy mask to losses
                        loss_focal += focal_loss(pred_mask, soft_mask)  #, entropy_mask=entropy_mask
                        loss_dice += dice_loss(pred_mask, soft_mask)   #, entropy_mask=entropy_mask
                        batch_iou = calc_iou(pred_mask.unsqueeze(0), soft_mask.unsqueeze(0))
                        loss_iou += F.mse_loss(iou_prediction.view(-1), batch_iou.view(-1), reduction='sum') / num_masks

                del  pred_masks, iou_predictions 
                del pred_stack, overlap_map, invert_overlap_map
                torch.cuda.empty_cache()
                # loss_dist = loss_dist / num_masks
                loss_dice = loss_dice / num_masks
                loss_focal = loss_focal / num_masks
                loss_sim  = loss_sim
             

                loss_total =  (20 * loss_focal +  loss_dice  + loss_iou +0.1*loss_sim )#      )#+    
                if watcher.is_outlier(loss_total):
                    continue
                fabric.backward(loss_total)

                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                torch.cuda.empty_cache()
                del  prompts, soft_masks

                batch_time.update(time.time() - end)
                end = time.time()

                focal_losses.update(loss_focal.item(), batch_size)
                dice_losses.update(loss_dice.item(), batch_size)
                iou_losses.update(loss_iou.item(), batch_size)
                total_losses.update(loss_total.item(), batch_size)
                sim_losses.update(loss_sim.item(), batch_size)
            

            if (iter+1) % match_interval==0:
                fabric.print(
                    f"Epoch [{epoch}] Iter [{iter + 1}/{len(train_dataloader)}] " f"| Time {batch_time.avg:.2f}s "
                    f"| Focal {focal_losses.avg:.4f} | Dice {dice_losses.avg:.4f} | "
                    f"IoU {iou_losses.avg:.4f} | Sim_loss {sim_losses.avg:.4f} | Total {total_losses.avg:.4f}"
                )
            if (iter+1) % eval_interval == 0:
                avg_means, _ = validate(fabric, cfg, model, val_dataloader, cfg.name, epoch)
                # avg_means = sum(entropy_means) / len(entropy_means)
                status = ""
                if avg_means > 0:  #best_ent
                    best_ent = avg_means
                    best_state = copy.deepcopy(model.state_dict())
                    torch.save(best_state, os.path.join(cfg.out_dir, "save", "best_model.pth"))
                    status = "Improved â†’ Model Saved"
                    no_improve_count = 0
                else:
                    model.load_state_dict(best_state)
                    no_improve_count += 1
                    status = f"Rollback ({no_improve_count})"

                # Write log entry
                with open(csv_path, "a", newline="") as f:
                    writer = csv.writer(f)
                    writer.writerow([epoch, iter + 1, avg_means, best_ent, status])

                fabric.print(f"Validation IoU={avg_means:.4f} | Best={best_ent:.4f} | {status}")

                # Stop if model fails to stabilize
                if no_improve_count >= max_patience:
                    fabric.print(f"Training stopped early after {no_improve_count} failed rollbacks.")
                    return





            
def configure_opt(cfg: Box, model: Model):

    def lr_lambda(step):
        if step < cfg.opt.warmup_steps:
            return step / cfg.opt.warmup_steps
        elif step < cfg.opt.steps[0]:
            return 1.0
        elif step < cfg.opt.steps[1]:
            return 1 / cfg.opt.decay_factor
        else:
            return 1 / (cfg.opt.decay_factor**2)

    optimizer = torch.optim.Adam(model.model.parameters(), lr=cfg.opt.learning_rate, weight_decay=cfg.opt.weight_decay)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    return optimizer, scheduler



def corrupt_main(cfg):
    for corrupt in cfg.corruptions:
        cfg.corrupt = corrupt
        cfg.out_name = corrupt
        torch.cuda.empty_cache()
        main(cfg)



def main(cfg: Box) -> int:

    gpu_ids = [str(i) for i in range(torch.cuda.device_count())]
    num_devices = len(gpu_ids)
    fabric = L.Fabric(accelerator="auto",
                      devices=num_devices,
                      strategy="auto",
                      loggers=[TensorBoardLogger(cfg.out_dir)])
    fabric.launch()
    fabric.seed_everything(1337 + fabric.global_rank)

    if fabric.global_rank == 0:
        os.makedirs(os.path.join(cfg.out_dir, "save"), exist_ok=True)
        create_csv(os.path.join(cfg.out_dir, "metrics.csv"), csv_head=cfg.csv_keys)

    with fabric.device:
        model = Model(cfg)
        model.setup()

    load_datasets = call_load_dataset(cfg)
    train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt = True)
    train_data = fabric._setup_dataloader(train_data)
    val_data = fabric._setup_dataloader(val_data)
    pt_data = fabric._setup_dataloader(pt_data)
    optimizer, scheduler = configure_opt(cfg, model)
    model, optimizer = fabric.setup(model, optimizer)

    

    auto_ckpt = None#_find_latest_checkpoint(os.path.join(cfg.out_dir, "save"))

    
    if auto_ckpt is not None:
        full_checkpoint = fabric.load(auto_ckpt)

        if isinstance(full_checkpoint, dict) and "model" in full_checkpoint:
            model.load_state_dict(full_checkpoint["model"])
            if "optimizer" in full_checkpoint:
                optimizer.load_state_dict(full_checkpoint["optimizer"])
        else:
            model.load_state_dict(full_checkpoint)
        loaded = True
        fabric.print(f"Resumed from explicit checkpoint: {cfg.model.ckpt}")
   
    init_iou = 0
    # print('-'*100)
    # print('\033[92mDirect test on the original SAM.\033[0m') 
    # init_iou, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)
    # print('-'*100)
    # del _     




    train_sam(cfg, fabric, model, optimizer, scheduler, train_data, val_data, init_iou)

    del model, train_data, val_data
















# ###############################################################################
# # ðŸ§  SAM2 Integration
# # ---------------------------------------------------------------------------
# # This section handles all SAM2-related components including:
# #   â€¢ Model prediction and mask generation
# #   â€¢ Prompt handling (points, boxes, embeddings, etc.)
# #   â€¢ Loss computation (focal, dice, entropy, IoU)
# #   â€¢ Device management (ensuring tensors stay on the same GPU)
# #   â€¢ Training and validation forward passes
# #
# # NOTE:
# #   Ensure all tensors (pred_mask, soft_mask, etc.) are moved to the same device
# #   before loss calculation to avoid cuda/cpu mismatches.
# ###############################################################################






# from sam2.sam2_image_predictor import SAM2ImagePredictor
# from sam2.build_sam import build_sam2
# # from peft import LoraConfig, get_peft_model

# model_cfg = "./configs/sam2/sam2_hiera_b+.yaml"
# checkpoint = "./pretrain/sam2_hiera_base_plus.pt"



# def sam2forward(img_tensor, prompts ,predictor):
    
#     images = img_tensor[0].permute(1, 2, 0).cpu().numpy()
#     with torch.no_grad():
#         predictor.set_image(images)
#         entropy_maps = []
#         pred_masks = []
#         for i in range(prompts[0][0].shape[0]):
#             mask_tuple, scores, logits = predictor.predict(
#                 point_coords=prompts[0][0][i].unsqueeze(0),      # single point
#                 point_labels=prompts[0][1][i].unsqueeze(0),      
#                 multimask_output=False           # only 1 mask
#             )

#             logits_full = F.interpolate(torch.tensor(logits).unsqueeze(0), size=(1024, 1024), mode='bilinear', align_corners=False)
#             soft_mask_full = torch.sigmoid(logits_full[0][0])

#             pred_mask = torch.sigmoid(soft_mask_full)

#             pred_masks.append(pred_mask)
            
#             entropy_map = entropy_map_calculate(pred_mask.unsqueeze(0))
#             entropy_maps.append(entropy_map)
    
#     return entropy_maps, pred_masks


# def sam2forward_bbox(img_tensor, prompts_boxes ,predictor):
    
#     images = img_tensor[0].permute(1, 2, 0).cpu().numpy()
#     with torch.no_grad():
#         predictor.set_image(images)
#         pred_masks = []
#         for i in range(prompts_boxes.shape[0]):
#             mask_tuple, scores, logits = predictor.predict(
#                 box=prompts_boxes[i].unsqueeze(0),      # single point
#                 multimask_output=False           # only 1 mask
#             )

#             pred_masks.append(mask_tuple[0])


#     return  pred_masks
        


# # #     return pred_masks, Iou_prediciton
# def pass_for_training(img_tensor, prompts, predictor):
#     """
#     Differentiable SAM2 forward pass for training with point prompts.
#     """

#     device = img_tensor.device
#     image = img_tensor[0].to(device)

#     # 1ï¸âƒ£ Encode image (keep gradients)
#     image_dict = predictor.model.image_encoder(image.unsqueeze(0))
#     image_embedding = image_dict["vision_features"]
#     image_pe = predictor.model.sam_prompt_encoder.get_dense_pe().to(device)

#     mask_decoder = predictor.model.sam_mask_decoder

    

#     # 3ï¸âƒ£ Prepare prompts
#     point_coords = prompts[0][0].to(device)
#     point_labels = prompts[0][1].to(device)

#     pred_masks = []
#     iou_predictions = []

#     # 4ï¸âƒ£ Loop over each point
#     for i in range(point_coords.shape[0]):
#         single_point = point_coords[i].unsqueeze(0)
#         single_label = point_labels[i].unsqueeze(0)

#         sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(
#             points=(single_point, single_label),
#             boxes=None,
#             masks=None
#         )

#         # 5ï¸âƒ£ Decode masks
     
#         mask_logits, iou_pred, mask_tokens_out, object_score_logit = predictor.model.sam_mask_decoder(
#             image_embeddings=image_embedding,
#             image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),
#             sparse_prompt_embeddings=sparse_embeddings,
#             dense_prompt_embeddings=dense_embeddings,
#             multimask_output=False,   # single mask output
#             repeat_image=False,
#         )

#         # 6ï¸âƒ£ Normalize masks
#         mask_logits = F.interpolate(mask_logits, size=(1024, 1024),
#                                     mode="bilinear", align_corners=False)
#         mask_probs = torch.sigmoid(mask_logits[0, 0])

#         pred_masks.append(mask_probs)
#         iou_predictions.append(iou_pred)

#     pred_masks = torch.stack(pred_masks)
#     iou_predictions = torch.stack(iou_predictions)

#     return pred_masks, iou_predictions





# def train_sam2(
#     cfg: Box,
#     fabric: L.Fabric,
#     model: Model,
#     optimizer: _FabricOptimizer,
#     scheduler: _FabricOptimizer,
#     train_dataloader: DataLoader,
#     val_dataloader: DataLoader,
#     target_pts,
# ):

#     focal_loss = FocalLoss()
#     dice_loss = DiceLoss()
#     max_iou = 0.
#     match_interval = cfg.match_interval

#     predictor = SAM2ImagePredictor(model)

#     for epoch in range(1, cfg.num_epochs + 1):
#         batch_time = AverageMeter()
#         data_time = AverageMeter()
#         focal_losses = AverageMeter()
#         dice_losses = AverageMeter()
#         iou_losses = AverageMeter()
#         total_losses = AverageMeter()
#         match_losses = AverageMeter()
#         end = time.time()
#         num_iter = len(train_dataloader)

#         eval_interval = int(len(train_dataloader) * 0.1) 

#         for iter, data in enumerate(train_dataloader):

#             data_time.update(time.time() - end)
#             images_weak, images_strong, bboxes, gt_masks, img_paths= data
#             del data

#             slice_step = 50
#             for i in range(0, len(gt_masks[0]), slice_step):
                
#                 gt_masks_new = gt_masks[0][i:i+slice_step].unsqueeze(0)
#                 prompts = get_prompts(cfg, bboxes, gt_masks_new)

#                 batch_size = images_weak.size(0)

#                 entropy_maps, preds = sam2forward(images_weak, prompts, predictor)
#                 pred_stack = torch.stack(preds, dim=0)
#                 pred_binary = (pred_stack > 0.5).float()
#                 overlap_count = pred_binary.sum(dim=0) 
#                 overlap_map = (overlap_count > 1).float()
#                 invert_overlap_map = 1.0 - overlap_map

                

#                 soft_masks = []
#                 bboxes = []
#                 point_list = []
#                 point_labels_list = []
#                 for i, (entr_map, pred) in enumerate(zip(entropy_maps, preds)):
#                     point_coords = prompts[0][0][i][:].unsqueeze(0)
#                     point_coords_lab = prompts[0][1][i][:].unsqueeze(0)

#                     entr_norm = (entr_map - entr_map.min()) / (entr_map.max() - entr_map.min() + 1e-8)
                    
#                     pred = (pred>0.5)
#                     pred_w_overlap = pred * invert_overlap_map

#                     ys, xs = torch.where(pred_w_overlap > 0.5)
#                     if len(xs) > 0 and len(ys) > 0:
#                         x_min, x_max = xs.min().item(), xs.max().item()
#                         y_min, y_max = ys.min().item(), ys.max().item()
#                         bboxes.append(torch.tensor([x_min, y_min , x_max, y_max], dtype=torch.float32))

#                         point_list.append(point_coords)
#                         point_labels_list.append(point_coords_lab)
                       
#                 if len(point_list) !=0:
#                     point_ = torch.cat(point_list).squeeze(1)
#                     point_labels_ = torch.cat(point_labels_list)
#                     new_prompts = [(point_, point_labels_)]
                
#                     bboxes = torch.stack(bboxes)

#                     soft_masks = sam2forward_bbox(images_weak, bboxes, predictor)

#                     pred_masks , iou_predictions = pass_for_training(images_strong, new_prompts,predictor )
                

#                     torch.cuda.empty_cache()

#                     num_masks = sum(len(pred_mask) for pred_mask in pred_masks)
#                     loss_focal = torch.tensor(0., device=fabric.device)
#                     loss_dice = torch.tensor(0., device=fabric.device)
#                     loss_iou = torch.tensor(0., device=fabric.device)

#                     for i, (pred_mask, soft_mask, iou_prediction) in enumerate(
#                             zip(pred_masks, soft_masks, iou_predictions  )
#                         ):
                            
#                             soft_mask = (torch.tensor(soft_mask) > 0.).float().unsqueeze(0)
#                             pred_mask = pred_mask.unsqueeze(0).to(soft_mask.device)
#                             iou_prediction = iou_prediction.to(soft_mask.device)
                        
                            
#                             # Apply entropy mask to losses
#                             loss_focal += focal_loss(pred_mask, soft_mask)  #, entropy_mask=entropy_mask
#                             loss_dice += dice_loss(pred_mask, soft_mask)   #, entropy_mask=entropy_mask
#                             batch_iou = calc_iou(pred_mask, soft_mask)
#                             loss_iou += F.mse_loss(iou_prediction, batch_iou, reduction='sum') / num_masks

                
#                     del  pred_masks, iou_predictions 
#                     # loss_dist = loss_dist / num_masks
#                     loss_dice = loss_dice #/ num_masks
#                     loss_focal = loss_focal #/ num_masks
#                     torch.cuda.empty_cache()


#                     loss_total =  20 * loss_focal +  loss_dice  + loss_iou #+ loss_iou  +  +



#                     fabric.backward(loss_total)

#                     optimizer.step()
#                     scheduler.step()
#                     optimizer.zero_grad()
#                     torch.cuda.empty_cache()
#                     del  prompts, soft_masks

#                     batch_time.update(time.time() - end)
#                     end = time.time()

#                     focal_losses.update(loss_focal.item(), batch_size)
#                     dice_losses.update(loss_dice.item(), batch_size)
#                     iou_losses.update(loss_iou.item(), batch_size)
#                     total_losses.update(loss_total.item(), batch_size)
                
#                     del loss_dice, loss_iou, loss_focal

#             if (iter+1) %int(eval_interval/10)==0:
#                 fabric.print(f'Epoch: [{epoch}][{iter + 1}/{len(train_dataloader)}]'
#                              f' | Time [{batch_time.val:.3f}s ({batch_time.avg:.3f}s)]'
#                              f' | Data [{data_time.val:.3f}s ({data_time.avg:.3f}s)]'
#                              f' | Focal Loss [{focal_losses.val:.4f} ({focal_losses.avg:.4f})]'
#                              f' | Dice Loss [{dice_losses.val:.4f} ({dice_losses.avg:.4f})]'
#                              f' | IoU Loss [{iou_losses.val:.4f} ({iou_losses.avg:.4f})]'
#                              f' | Total Loss [{total_losses.val:.4f} ({total_losses.avg:.4f})]')

#             if (iter+1)%eval_interval == 0:
#                 iou, _, = validate_sam2(fabric, cfg, model, val_dataloader, name=cfg.name, epoch=0)
#                 del iou
#             torch.cuda.empty_cache()
            
#         # if epoch % cfg.eval_interval == 0:
#         #     iou, _= validate(fabric, cfg, model, val_dataloader, cfg.name, epoch)
#         #     # if iou > max_iou:
#         #     #     state = {"model": model, "optimizer": optimizer}
#         #     #     fabric.save(os.path.join(cfg.out_dir, "save", "best-ckpt.pth"), state)
#         #     #     max_iou = iou
#         #     del iou  

# def configure_opt2(cfg: Box, model: Model):

#     def lr_lambda(step):
#         if step < cfg.opt.warmup_steps:
#             return step / cfg.opt.warmup_steps
#         elif step < cfg.opt.steps[0]:
#             return 1.0
#         elif step < cfg.opt.steps[1]:
#             return 1 / cfg.opt.decay_factor
#         else:
#             return 1 / (cfg.opt.decay_factor**2)

#     # optimize only trainable params (e.g., LoRA)
#     trainable_params = (p for p in model.parameters() if p.requires_grad)
#     optimizer = torch.optim.Adam(trainable_params, lr=cfg.opt.learning_rate, weight_decay=cfg.opt.weight_decay)
#     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

#     return optimizer, scheduler


# def main2(cfg: Box) -> int:
#     gpu_ids = [str(i) for i in range(torch.cuda.device_count())]
#     num_devices = len(gpu_ids)
#     fabric = L.Fabric(accelerator="auto",
#                       devices=num_devices,
#                       strategy="auto",
#                       loggers=[TensorBoardLogger(cfg.out_dir)])
#     fabric.launch()
#     fabric.seed_everything(1337 + fabric.global_rank)

#     if fabric.global_rank == 0:
#         os.makedirs(os.path.join(cfg.out_dir, "save"), exist_ok=True)
#         create_csv(os.path.join(cfg.out_dir, "metrics.csv"), csv_head=cfg.csv_keys)

#     with fabric.device:
#         model = build_sam2(model_cfg, checkpoint, mode='train')
#     encoder = model.image_encoder
#     lora_config = LoraConfig(
#         r=4,                   # rank
#         lora_alpha=16,
#         target_modules=["qkv"],  # Hiera merges q,k,v in one linear layer
#         lora_dropout=0.05,
#         bias="none",
#     )
#     model = get_peft_model(model, lora_config)

#     # model.image_encoder = encoder


#     load_datasets = call_load_dataset(cfg)
#     train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt = True)
#     train_data = fabric._setup_dataloader(train_data)
#     val_data = fabric._setup_dataloader(val_data)
#     pt_data = fabric._setup_dataloader(pt_data)
#     optimizer, scheduler = configure_opt2(cfg, model)
#     model, optimizer = fabric.setup(model, optimizer)

#     if cfg.resume and cfg.model.ckpt is not None:
#         full_checkpoint = fabric.load(checkpoint)
#         model.load_state_dict(full_checkpoint["model"])
#         optimizer.load_state_dict(full_checkpoint["optimizer"])


#     # print('-'*100)
#     # print('\033[92mDirect test on the original SAM.\033[0m') 
#     # _, _, = validate_sam2(fabric, cfg, model, val_data, name=cfg.name, epoch=0)
#     # print('-'*100)
#     # del _     


#     train_sam2(cfg, fabric, model, optimizer, scheduler, train_data, val_data, pt_data)

#     del model, train_data, val_data





def parse_args():
    parser = argparse.ArgumentParser(description='Train a detector')
    parser.add_argument('--cfg', help='train config file path')
    parser.add_argument('--prompt', help='the type of prompt')
    parser.add_argument('--num_points',type=int, help='the number of points')
    parser.add_argument('--out_dir', help='the dir to save logs and models')
    parser.add_argument('--load_type', help='the dir to save logs and models')      
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    print(torch.cuda.current_device())
    torch.cuda.empty_cache()
    torch.set_float32_matmul_precision('high')
    args = parse_args()

    exec(f'from {args.cfg} import cfg')

    # transfer the args to a dict
    args_dict = vars(args)
    cfg.merge_update(args_dict)
    print(cfg.model.backend)

    if cfg.model.backend == 'sam':
        main(cfg)
    elif cfg.model.backend == 'sam2':
        main2(cfg)
    torch.cuda.empty_cache()







